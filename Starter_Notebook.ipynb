{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "wApqhsthcQZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0f5qGxCG1L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip3 install gym 1>/dev/null\n",
        "!pip3 install keras-rl 1>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "9f206fe4-a3f0-4657-b491-a8100fd99d8a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557296212,
          "user_tz": -60,
          "elapsed": 11780,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/arpat/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/install_ngrok.sh 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/install_novnc.sh 1>/dev/null"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 59, done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 59 (delta 28), reused 37 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a9696716-3c14-491f-8a45-641adf9f5293",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557421730,
          "user_tz": -60,
          "elapsed": 7513,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "get_ipython().system_raw('/opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
            "XWindow Link: https://8087f06b.ngrok.io\n",
            "Full control one-time password: 41252877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "#get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d7adbcb-d70a-48d3-c615-00da22c99367",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557312292,
          "user_tz": -60,
          "elapsed": 1531,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc75ccb5-890a-4296-c78b-c9c83752db4c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557313615,
          "user_tz": -60,
          "elapsed": 513,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "88b5536c-16c7-4bed-b88e-b641d38f3cb7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557314118,
          "user_tz": -60,
          "elapsed": 430,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "model.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "model.add(layers.Dense(num_actions))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=model,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "outputId": "af9d0a01-0029-4591-c272-d81613aa1ce1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557327686,
          "user_tz": -60,
          "elapsed": 9665,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  28/1000: episode: 1, duration: 0.278s, episode steps: 28, steps per second: 101, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.405, 0.901], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  39/1000: episode: 2, duration: 0.015s, episode steps: 11, steps per second: 710, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.124 [-0.800, 1.403], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  67/1000: episode: 3, duration: 0.044s, episode steps: 28, steps per second: 642, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.019 [-1.016, 1.562], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  82/1000: episode: 4, duration: 0.022s, episode steps: 15, steps per second: 695, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.110 [-1.495, 0.773], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  98/1000: episode: 5, duration: 0.029s, episode steps: 16, steps per second: 544, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.080 [-1.038, 1.760], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 108/1000: episode: 6, duration: 0.370s, episode steps: 10, steps per second: 27, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-2.092, 1.415], loss: 0.587120, mean_absolute_error: 0.649105, mean_q: 0.258459\n",
            " 131/1000: episode: 7, duration: 0.212s, episode steps: 23, steps per second: 109, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.077 [-1.030, 1.968], loss: 0.533817, mean_absolute_error: 0.658998, mean_q: 0.254307\n",
            " 199/1000: episode: 8, duration: 0.586s, episode steps: 68, steps per second: 116, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.153 [-1.475, 1.744], loss: 0.383687, mean_absolute_error: 0.662270, mean_q: 0.496849\n",
            " 214/1000: episode: 9, duration: 0.134s, episode steps: 15, steps per second: 112, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.093 [-1.363, 2.264], loss: 0.314867, mean_absolute_error: 0.674371, mean_q: 0.626108\n",
            " 238/1000: episode: 10, duration: 0.229s, episode steps: 24, steps per second: 105, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.400, 0.983], loss: 0.298106, mean_absolute_error: 0.707332, mean_q: 0.731444\n",
            " 252/1000: episode: 11, duration: 0.133s, episode steps: 14, steps per second: 105, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.115 [-0.735, 1.251], loss: 0.281186, mean_absolute_error: 0.719154, mean_q: 0.786169\n",
            " 272/1000: episode: 12, duration: 0.209s, episode steps: 20, steps per second: 96, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.064 [-1.144, 1.820], loss: 0.278380, mean_absolute_error: 0.753342, mean_q: 0.869809\n",
            " 289/1000: episode: 13, duration: 0.173s, episode steps: 17, steps per second: 98, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.083 [-1.363, 0.770], loss: 0.282528, mean_absolute_error: 0.795683, mean_q: 0.948487\n",
            " 305/1000: episode: 14, duration: 0.177s, episode steps: 16, steps per second: 90, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.957, 1.698], loss: 0.267644, mean_absolute_error: 0.813832, mean_q: 1.015541\n",
            " 324/1000: episode: 15, duration: 0.195s, episode steps: 19, steps per second: 97, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.074 [-1.644, 0.937], loss: 0.269517, mean_absolute_error: 0.851427, mean_q: 1.085157\n",
            " 348/1000: episode: 16, duration: 0.248s, episode steps: 24, steps per second: 97, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.057 [-1.415, 0.819], loss: 0.280516, mean_absolute_error: 0.887748, mean_q: 1.154465\n",
            " 373/1000: episode: 17, duration: 0.246s, episode steps: 25, steps per second: 102, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.029 [-1.594, 2.304], loss: 0.274691, mean_absolute_error: 0.908914, mean_q: 1.223855\n",
            " 396/1000: episode: 18, duration: 0.218s, episode steps: 23, steps per second: 105, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.032 [-1.028, 1.591], loss: 0.283822, mean_absolute_error: 0.972901, mean_q: 1.341998\n",
            " 409/1000: episode: 19, duration: 0.125s, episode steps: 13, steps per second: 104, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.094 [-0.997, 1.700], loss: 0.285749, mean_absolute_error: 0.986262, mean_q: 1.385989\n",
            " 460/1000: episode: 20, duration: 0.475s, episode steps: 51, steps per second: 107, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.113 [-1.721, 1.734], loss: 0.285400, mean_absolute_error: 1.050468, mean_q: 1.517143\n",
            " 469/1000: episode: 21, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.794, 2.838], loss: 0.295643, mean_absolute_error: 1.113310, mean_q: 1.605643\n",
            " 487/1000: episode: 22, duration: 0.180s, episode steps: 18, steps per second: 100, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.080 [-0.753, 1.184], loss: 0.294526, mean_absolute_error: 1.147158, mean_q: 1.686314\n",
            " 521/1000: episode: 23, duration: 0.321s, episode steps: 34, steps per second: 106, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.012 [-1.388, 1.760], loss: 0.299795, mean_absolute_error: 1.198553, mean_q: 1.786629\n",
            " 548/1000: episode: 24, duration: 0.258s, episode steps: 27, steps per second: 105, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.095 [-0.761, 1.612], loss: 0.298498, mean_absolute_error: 1.247556, mean_q: 1.896567\n",
            " 565/1000: episode: 25, duration: 0.159s, episode steps: 17, steps per second: 107, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.063 [-1.734, 2.560], loss: 0.304770, mean_absolute_error: 1.277589, mean_q: 1.979234\n",
            " 577/1000: episode: 26, duration: 0.118s, episode steps: 12, steps per second: 102, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.778, 1.034], loss: 0.321039, mean_absolute_error: 1.333650, mean_q: 2.068679\n",
            " 617/1000: episode: 27, duration: 0.364s, episode steps: 40, steps per second: 110, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.012 [-1.404, 2.112], loss: 0.331344, mean_absolute_error: 1.374753, mean_q: 2.149969\n",
            " 632/1000: episode: 28, duration: 0.138s, episode steps: 15, steps per second: 108, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.101 [-1.866, 1.130], loss: 0.345967, mean_absolute_error: 1.446673, mean_q: 2.289508\n",
            " 646/1000: episode: 29, duration: 0.153s, episode steps: 14, steps per second: 92, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.080 [-1.594, 2.483], loss: 0.338541, mean_absolute_error: 1.453053, mean_q: 2.320751\n",
            " 682/1000: episode: 30, duration: 0.333s, episode steps: 36, steps per second: 108, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.014 [-1.179, 1.763], loss: 0.357431, mean_absolute_error: 1.506209, mean_q: 2.406329\n",
            " 693/1000: episode: 31, duration: 0.104s, episode steps: 11, steps per second: 106, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.088 [-1.413, 2.085], loss: 0.366619, mean_absolute_error: 1.577834, mean_q: 2.566391\n",
            " 713/1000: episode: 32, duration: 0.202s, episode steps: 20, steps per second: 99, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.063 [-1.577, 2.517], loss: 0.366821, mean_absolute_error: 1.599138, mean_q: 2.598984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 736/1000: episode: 33, duration: 0.217s, episode steps: 23, steps per second: 106, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.084 [-0.821, 1.438], loss: 0.381029, mean_absolute_error: 1.652247, mean_q: 2.736923\n",
            " 761/1000: episode: 34, duration: 0.223s, episode steps: 25, steps per second: 112, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-1.018, 2.032], loss: 0.397556, mean_absolute_error: 1.702432, mean_q: 2.826024\n",
            " 774/1000: episode: 35, duration: 0.114s, episode steps: 13, steps per second: 114, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.098 [-0.965, 1.479], loss: 0.412570, mean_absolute_error: 1.743468, mean_q: 2.908941\n",
            " 790/1000: episode: 36, duration: 0.153s, episode steps: 16, steps per second: 105, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.078 [-1.605, 2.554], loss: 0.417014, mean_absolute_error: 1.776775, mean_q: 2.947880\n",
            " 808/1000: episode: 37, duration: 0.153s, episode steps: 18, steps per second: 118, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.075 [-0.833, 1.524], loss: 0.493049, mean_absolute_error: 1.831416, mean_q: 3.058867\n",
            " 828/1000: episode: 38, duration: 0.198s, episode steps: 20, steps per second: 101, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.070 [-1.171, 1.989], loss: 0.475898, mean_absolute_error: 1.862267, mean_q: 3.110988\n",
            " 838/1000: episode: 39, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.135, 2.059], loss: 0.462931, mean_absolute_error: 1.905175, mean_q: 3.218812\n",
            " 850/1000: episode: 40, duration: 0.126s, episode steps: 12, steps per second: 96, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.129 [-1.549, 2.594], loss: 0.477450, mean_absolute_error: 1.908791, mean_q: 3.250958\n",
            " 873/1000: episode: 41, duration: 0.217s, episode steps: 23, steps per second: 106, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.039 [-1.344, 2.197], loss: 0.500012, mean_absolute_error: 1.971761, mean_q: 3.343683\n",
            " 884/1000: episode: 42, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.106 [-1.163, 1.797], loss: 0.496297, mean_absolute_error: 1.997284, mean_q: 3.401375\n",
            " 901/1000: episode: 43, duration: 0.172s, episode steps: 17, steps per second: 99, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.085 [-1.398, 0.929], loss: 0.588722, mean_absolute_error: 2.066750, mean_q: 3.475468\n",
            " 911/1000: episode: 44, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.119 [-1.217, 1.949], loss: 0.556383, mean_absolute_error: 2.058134, mean_q: 3.504348\n",
            " 927/1000: episode: 45, duration: 0.157s, episode steps: 16, steps per second: 102, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.067 [-1.581, 2.547], loss: 0.526275, mean_absolute_error: 2.069312, mean_q: 3.529783\n",
            " 939/1000: episode: 46, duration: 0.106s, episode steps: 12, steps per second: 114, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.089 [-1.024, 1.718], loss: 0.502824, mean_absolute_error: 2.108737, mean_q: 3.619946\n",
            " 973/1000: episode: 47, duration: 0.315s, episode steps: 34, steps per second: 108, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.054 [-0.757, 1.667], loss: 0.592728, mean_absolute_error: 2.172415, mean_q: 3.732088\n",
            "done, took 9.270 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "9bfa9fae-89c3-499a-e050-971c180fdb9b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527557328761,
          "user_tz": -60,
          "elapsed": 966,
          "user": {
            "displayName": "Arun Patel",
            "photoUrl": "//lh3.googleusercontent.com/-K6BBoIySv0g/AAAAAAAAAAI/AAAAAAAAABg/sW1JdvMqbvA/s50-c-k-no/photo.jpg",
            "userId": "113600916176777473265"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 8.000, steps: 8\n",
            "Episode 2: reward: 10.000, steps: 10\n",
            "Episode 3: reward: 9.000, steps: 9\n",
            "Episode 4: reward: 10.000, steps: 10\n",
            "Episode 5: reward: 8.000, steps: 8\n",
            "Episode 6: reward: 9.000, steps: 9\n",
            "Episode 7: reward: 10.000, steps: 10\n",
            "Episode 8: reward: 10.000, steps: 10\n",
            "Episode 9: reward: 9.000, steps: 9\n",
            "Episode 10: reward: 9.000, steps: 9\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 10.000, steps: 10\n",
            "Episode 13: reward: 9.000, steps: 9\n",
            "Episode 14: reward: 9.000, steps: 9\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 10.000, steps: 10\n",
            "Episode 17: reward: 10.000, steps: 10\n",
            "Episode 18: reward: 9.000, steps: 9\n",
            "Episode 19: reward: 10.000, steps: 10\n",
            "Episode 20: reward: 10.000, steps: 10\n",
            "Episode 21: reward: 9.000, steps: 9\n",
            "Episode 22: reward: 8.000, steps: 8\n",
            "Episode 23: reward: 8.000, steps: 8\n",
            "Episode 24: reward: 8.000, steps: 8\n",
            "Episode 25: reward: 10.000, steps: 10\n",
            "Episode 26: reward: 10.000, steps: 10\n",
            "Episode 27: reward: 10.000, steps: 10\n",
            "Episode 28: reward: 10.000, steps: 10\n",
            "Episode 29: reward: 9.000, steps: 9\n",
            "Episode 30: reward: 9.000, steps: 9\n",
            "Episode 31: reward: 9.000, steps: 9\n",
            "Episode 32: reward: 10.000, steps: 10\n",
            "Episode 33: reward: 11.000, steps: 11\n",
            "Episode 34: reward: 9.000, steps: 9\n",
            "Episode 35: reward: 9.000, steps: 9\n",
            "Episode 36: reward: 8.000, steps: 8\n",
            "Episode 37: reward: 10.000, steps: 10\n",
            "Episode 38: reward: 9.000, steps: 9\n",
            "Episode 39: reward: 9.000, steps: 9\n",
            "Episode 40: reward: 9.000, steps: 9\n",
            "Episode 41: reward: 10.000, steps: 10\n",
            "Episode 42: reward: 10.000, steps: 10\n",
            "Episode 43: reward: 10.000, steps: 10\n",
            "Episode 44: reward: 10.000, steps: 10\n",
            "Episode 45: reward: 10.000, steps: 10\n",
            "Episode 46: reward: 9.000, steps: 9\n",
            "Episode 47: reward: 9.000, steps: 9\n",
            "Episode 48: reward: 9.000, steps: 9\n",
            "Episode 49: reward: 8.000, steps: 8\n",
            "Episode 50: reward: 10.000, steps: 10\n",
            "mean: 9.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1oqpa_UIMO_x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}