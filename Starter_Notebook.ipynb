{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "wApqhsthcQZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0f5qGxCG1L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e1100335-d0b1-478e-9b37-3ed19d08feb5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527015055764,
          "user_tz": -60,
          "elapsed": 4629,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install gym\n",
        "!pip3 install keras-rl"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\r\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.3)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (0.19.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04da6f7f-57a7-4d70-c3d1-d7f073e7d6fb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527015057497,
          "user_tz": -60,
          "elapsed": 437,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "89c1afd8-060b-4da1-c4f1-dbb9045ac8cc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527015058081,
          "user_tz": -60,
          "elapsed": 500,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "model.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "model.add(layers.Dense(4,activation='relu'))\n",
        "\n",
        "# keep this as output layer\n",
        "model.add(layers.Dense(num_actions))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=model,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "bf9be134-5348-44d3-f0f7-728a56980e25",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527015065891,
          "user_tz": -60,
          "elapsed": 4184,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  13/1000: episode: 1, duration: 0.053s, episode steps: 13, steps per second: 243, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.086 [-1.949, 1.205], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  33/1000: episode: 2, duration: 0.013s, episode steps: 20, steps per second: 1485, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-0.937, 0.565], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  58/1000: episode: 3, duration: 0.017s, episode steps: 25, steps per second: 1501, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.105 [-1.379, 0.430], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 111/1000: episode: 4, duration: 0.331s, episode steps: 53, steps per second: 160, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.058 [-0.969, 1.027], loss: 0.511733, mean_absolute_error: 0.545802, mean_q: 0.042163\n",
            " 127/1000: episode: 5, duration: 0.056s, episode steps: 16, steps per second: 286, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.593, 1.015], loss: 0.437938, mean_absolute_error: 0.521190, mean_q: 0.148291\n",
            " 139/1000: episode: 6, duration: 0.044s, episode steps: 12, steps per second: 274, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.119 [-0.600, 1.245], loss: 0.387612, mean_absolute_error: 0.516348, mean_q: 0.230768\n",
            " 154/1000: episode: 7, duration: 0.057s, episode steps: 15, steps per second: 263, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.087 [-0.819, 1.271], loss: 0.353349, mean_absolute_error: 0.522473, mean_q: 0.317214\n",
            " 172/1000: episode: 8, duration: 0.068s, episode steps: 18, steps per second: 265, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.050 [-1.616, 1.008], loss: 0.322527, mean_absolute_error: 0.539474, mean_q: 0.389861\n",
            " 210/1000: episode: 9, duration: 0.125s, episode steps: 38, steps per second: 303, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.149 [-1.478, 0.572], loss: 0.292257, mean_absolute_error: 0.579762, mean_q: 0.528963\n",
            " 237/1000: episode: 10, duration: 0.100s, episode steps: 27, steps per second: 270, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.023 [-1.346, 0.818], loss: 0.270246, mean_absolute_error: 0.624972, mean_q: 0.686035\n",
            " 250/1000: episode: 11, duration: 0.046s, episode steps: 13, steps per second: 282, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.747, 1.492], loss: 0.272704, mean_absolute_error: 0.678686, mean_q: 0.771465\n",
            " 266/1000: episode: 12, duration: 0.051s, episode steps: 16, steps per second: 314, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.105 [-0.996, 1.805], loss: 0.259741, mean_absolute_error: 0.689063, mean_q: 0.847447\n",
            " 295/1000: episode: 13, duration: 0.092s, episode steps: 29, steps per second: 314, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.103 [-1.316, 0.566], loss: 0.261276, mean_absolute_error: 0.742683, mean_q: 0.927383\n",
            " 320/1000: episode: 14, duration: 0.086s, episode steps: 25, steps per second: 290, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.033 [-2.281, 1.517], loss: 0.246915, mean_absolute_error: 0.786434, mean_q: 1.056262\n",
            " 337/1000: episode: 15, duration: 0.061s, episode steps: 17, steps per second: 277, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.090 [-1.847, 1.003], loss: 0.249557, mean_absolute_error: 0.836705, mean_q: 1.146490\n",
            " 359/1000: episode: 16, duration: 0.070s, episode steps: 22, steps per second: 314, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.097 [-0.565, 1.301], loss: 0.243936, mean_absolute_error: 0.877096, mean_q: 1.236897\n",
            " 373/1000: episode: 17, duration: 0.049s, episode steps: 14, steps per second: 285, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.125 [-0.952, 1.763], loss: 0.237283, mean_absolute_error: 0.912921, mean_q: 1.316770\n",
            " 385/1000: episode: 18, duration: 0.050s, episode steps: 12, steps per second: 238, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.122 [-2.207, 1.375], loss: 0.242543, mean_absolute_error: 0.946546, mean_q: 1.394652\n",
            " 399/1000: episode: 19, duration: 0.055s, episode steps: 14, steps per second: 255, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-1.929, 1.144], loss: 0.243362, mean_absolute_error: 0.984618, mean_q: 1.457332\n",
            " 417/1000: episode: 20, duration: 0.060s, episode steps: 18, steps per second: 299, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.089 [-1.273, 0.785], loss: 0.241632, mean_absolute_error: 1.021462, mean_q: 1.538970\n",
            " 443/1000: episode: 21, duration: 0.092s, episode steps: 26, steps per second: 284, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.069 [-1.592, 0.786], loss: 0.233620, mean_absolute_error: 1.071699, mean_q: 1.654409\n",
            " 491/1000: episode: 22, duration: 0.166s, episode steps: 48, steps per second: 290, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.102 [-1.915, 0.961], loss: 0.229646, mean_absolute_error: 1.168241, mean_q: 1.846328\n",
            " 510/1000: episode: 23, duration: 0.079s, episode steps: 19, steps per second: 241, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.075 [-1.551, 0.811], loss: 0.217772, mean_absolute_error: 1.253111, mean_q: 2.051148\n",
            " 528/1000: episode: 24, duration: 0.057s, episode steps: 18, steps per second: 315, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.103 [-0.624, 1.344], loss: 0.218031, mean_absolute_error: 1.307558, mean_q: 2.161048\n",
            " 572/1000: episode: 25, duration: 0.136s, episode steps: 44, steps per second: 323, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.617, 1.427], loss: 0.210349, mean_absolute_error: 1.392887, mean_q: 2.355374\n",
            " 610/1000: episode: 26, duration: 0.131s, episode steps: 38, steps per second: 291, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-1.103, 0.824], loss: 0.209405, mean_absolute_error: 1.517631, mean_q: 2.591110\n",
            " 631/1000: episode: 27, duration: 0.075s, episode steps: 21, steps per second: 280, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.060 [-1.319, 0.807], loss: 0.198682, mean_absolute_error: 1.605598, mean_q: 2.781406\n",
            " 661/1000: episode: 28, duration: 0.107s, episode steps: 30, steps per second: 279, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: -0.057 [-2.663, 1.592], loss: 0.201940, mean_absolute_error: 1.692515, mean_q: 2.967937\n",
            " 669/1000: episode: 29, duration: 0.037s, episode steps: 8, steps per second: 215, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.128 [-2.189, 1.369], loss: 0.198169, mean_absolute_error: 1.752720, mean_q: 3.110313\n",
            " 679/1000: episode: 30, duration: 0.036s, episode steps: 10, steps per second: 280, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.514, 1.575], loss: 0.173336, mean_absolute_error: 1.772436, mean_q: 3.175992\n",
            " 696/1000: episode: 31, duration: 0.058s, episode steps: 17, steps per second: 292, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.057 [-2.270, 1.412], loss: 0.220010, mean_absolute_error: 1.839505, mean_q: 3.292112\n",
            " 719/1000: episode: 32, duration: 0.080s, episode steps: 23, steps per second: 286, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.078 [-0.755, 1.472], loss: 0.211661, mean_absolute_error: 1.890548, mean_q: 3.398137\n",
            " 735/1000: episode: 33, duration: 0.065s, episode steps: 16, steps per second: 247, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.089 [-0.962, 1.574], loss: 0.253409, mean_absolute_error: 1.973799, mean_q: 3.535411\n",
            " 757/1000: episode: 34, duration: 0.070s, episode steps: 22, steps per second: 314, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.049 [-2.311, 1.512], loss: 0.212534, mean_absolute_error: 2.013766, mean_q: 3.661789\n",
            " 774/1000: episode: 35, duration: 0.054s, episode steps: 17, steps per second: 314, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.090 [-1.154, 0.746], loss: 0.281020, mean_absolute_error: 2.100538, mean_q: 3.820303\n",
            " 800/1000: episode: 36, duration: 0.097s, episode steps: 26, steps per second: 268, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.027 [-1.654, 1.023], loss: 0.273143, mean_absolute_error: 2.169067, mean_q: 3.930760\n",
            " 822/1000: episode: 37, duration: 0.071s, episode steps: 22, steps per second: 312, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.100 [-1.200, 0.550], loss: 0.195137, mean_absolute_error: 2.221672, mean_q: 4.097437\n",
            " 856/1000: episode: 38, duration: 0.119s, episode steps: 34, steps per second: 287, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: 0.017 [-1.551, 2.332], loss: 0.283869, mean_absolute_error: 2.341130, mean_q: 4.294387\n",
            " 867/1000: episode: 39, duration: 0.048s, episode steps: 11, steps per second: 227, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-1.427, 0.777], loss: 0.345109, mean_absolute_error: 2.440858, mean_q: 4.456882\n",
            " 884/1000: episode: 40, duration: 0.055s, episode steps: 17, steps per second: 308, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.104 [-1.932, 0.984], loss: 0.397499, mean_absolute_error: 2.494261, mean_q: 4.556909\n",
            " 903/1000: episode: 41, duration: 0.061s, episode steps: 19, steps per second: 313, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.086 [-2.107, 1.184], loss: 0.376086, mean_absolute_error: 2.558290, mean_q: 4.688280\n",
            " 920/1000: episode: 42, duration: 0.054s, episode steps: 17, steps per second: 314, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.069 [-1.018, 1.764], loss: 0.358830, mean_absolute_error: 2.607292, mean_q: 4.791167\n",
            " 944/1000: episode: 43, duration: 0.090s, episode steps: 24, steps per second: 265, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.038 [-2.492, 1.597], loss: 0.444112, mean_absolute_error: 2.691102, mean_q: 4.910183\n",
            " 976/1000: episode: 44, duration: 0.112s, episode steps: 32, steps per second: 287, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.093 [-2.061, 0.833], loss: 0.500672, mean_absolute_error: 2.790283, mean_q: 5.082868\n",
            " 988/1000: episode: 45, duration: 0.049s, episode steps: 12, steps per second: 247, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.125 [-2.008, 1.163], loss: 0.505663, mean_absolute_error: 2.846256, mean_q: 5.199928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "done, took 3.561 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "5333a380-00e7-4d6f-96b9-85846afa6068",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527015066720,
          "user_tz": -60,
          "elapsed": 789,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=25, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 25 episodes ...\n",
            "Episode 1: reward: 39.000, steps: 39\n",
            "Episode 2: reward: 30.000, steps: 30\n",
            "Episode 3: reward: 36.000, steps: 36\n",
            "Episode 4: reward: 29.000, steps: 29\n",
            "Episode 5: reward: 34.000, steps: 34\n",
            "Episode 6: reward: 35.000, steps: 35\n",
            "Episode 7: reward: 11.000, steps: 11\n",
            "Episode 8: reward: 32.000, steps: 32\n",
            "Episode 9: reward: 48.000, steps: 48\n",
            "Episode 10: reward: 14.000, steps: 14\n",
            "Episode 11: reward: 37.000, steps: 37\n",
            "Episode 12: reward: 12.000, steps: 12\n",
            "Episode 13: reward: 33.000, steps: 33\n",
            "Episode 14: reward: 35.000, steps: 35\n",
            "Episode 15: reward: 14.000, steps: 14\n",
            "Episode 16: reward: 31.000, steps: 31\n",
            "Episode 17: reward: 12.000, steps: 12\n",
            "Episode 18: reward: 34.000, steps: 34\n",
            "Episode 19: reward: 40.000, steps: 40\n",
            "Episode 20: reward: 32.000, steps: 32\n",
            "Episode 21: reward: 28.000, steps: 28\n",
            "Episode 22: reward: 10.000, steps: 10\n",
            "Episode 23: reward: 35.000, steps: 35\n",
            "Episode 24: reward: 13.000, steps: 13\n",
            "Episode 25: reward: 32.000, steps: 32\n",
            "mean: 28.24\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}