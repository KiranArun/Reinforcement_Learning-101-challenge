{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "9fb6833e-ff6f-4233-975f-8d55a7f8acbf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527586496882,
          "user_tz": -60,
          "elapsed": 79603,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Reinforcement_Learning-101-challenge': No such file or directory\n",
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 75, done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 75 (delta 39), reused 45 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), done.\n",
            "Extracting templates from packages: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4e255701-4ff5-4703-a05d-dde9b83c39fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527588937046,
          "user_tz": -60,
          "elapsed": 13623,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "import os\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "# hack line below to get vnc to start as ipython subprocess\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XWindow Link: http://3f1d7a27.ngrok.io\n",
            "Full control one-time password: 15267371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 150\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 5e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 5e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9264d0db-d832-45d4-f1ae-39ccc32262cb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527588271209,
          "user_tz": -60,
          "elapsed": 542,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "cdd3c883-9518-4364-eacc-ffe11319616b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527588271904,
          "user_tz": -60,
          "elapsed": 620,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1001
        },
        "outputId": "5839fb35-90fd-430f-a32f-ab9dab3c8857",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527588281289,
          "user_tz": -60,
          "elapsed": 5306,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  17/1000: episode: 1, duration: 0.051s, episode steps: 17, steps per second: 335, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.082 [-1.020, 1.818], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  33/1000: episode: 2, duration: 0.010s, episode steps: 16, steps per second: 1645, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.097 [-1.220, 2.101], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  52/1000: episode: 3, duration: 0.011s, episode steps: 19, steps per second: 1681, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.073 [-1.006, 1.841], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  63/1000: episode: 4, duration: 0.007s, episode steps: 11, steps per second: 1486, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.180, 1.389], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  76/1000: episode: 5, duration: 0.008s, episode steps: 13, steps per second: 1605, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.093 [-1.523, 0.985], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  89/1000: episode: 6, duration: 0.008s, episode steps: 13, steps per second: 1592, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.096 [-2.773, 1.768], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 105/1000: episode: 7, duration: 0.279s, episode steps: 16, steps per second: 57, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.089 [-2.631, 1.569], loss: 0.591488, mean_absolute_error: 0.610155, mean_q: 0.209077\n",
            " 118/1000: episode: 8, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.127 [-1.902, 1.032], loss: 0.428929, mean_absolute_error: 0.597485, mean_q: 0.499096\n",
            " 127/1000: episode: 9, duration: 0.043s, episode steps: 9, steps per second: 210, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.717, 2.863], loss: 0.360549, mean_absolute_error: 0.659687, mean_q: 0.757630\n",
            " 138/1000: episode: 10, duration: 0.052s, episode steps: 11, steps per second: 213, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.136 [-2.200, 1.333], loss: 0.335747, mean_absolute_error: 0.738592, mean_q: 0.977763\n",
            " 154/1000: episode: 11, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.084 [-1.853, 1.129], loss: 0.341318, mean_absolute_error: 0.874589, mean_q: 1.270128\n",
            " 167/1000: episode: 12, duration: 0.059s, episode steps: 13, steps per second: 220, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.129 [-1.349, 0.615], loss: 0.364702, mean_absolute_error: 1.052751, mean_q: 1.645291\n",
            " 182/1000: episode: 13, duration: 0.065s, episode steps: 15, steps per second: 231, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.127 [-2.423, 1.336], loss: 0.412389, mean_absolute_error: 1.259998, mean_q: 2.003963\n",
            " 197/1000: episode: 14, duration: 0.071s, episode steps: 15, steps per second: 212, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.080 [-2.242, 1.402], loss: 0.468155, mean_absolute_error: 1.474995, mean_q: 2.403093\n",
            " 209/1000: episode: 15, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.105 [-1.961, 1.137], loss: 0.524017, mean_absolute_error: 1.635378, mean_q: 2.664198\n",
            " 226/1000: episode: 16, duration: 0.076s, episode steps: 17, steps per second: 223, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.078 [-1.013, 1.519], loss: 0.600218, mean_absolute_error: 1.795121, mean_q: 2.906450\n",
            " 246/1000: episode: 17, duration: 0.094s, episode steps: 20, steps per second: 214, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.090 [-1.166, 2.092], loss: 0.604617, mean_absolute_error: 1.879768, mean_q: 3.026305\n",
            " 275/1000: episode: 18, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.030 [-1.641, 1.164], loss: 0.586318, mean_absolute_error: 2.021794, mean_q: 3.299177\n",
            " 291/1000: episode: 19, duration: 0.070s, episode steps: 16, steps per second: 229, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.111 [-2.078, 1.142], loss: 0.644606, mean_absolute_error: 2.229487, mean_q: 3.690441\n",
            " 320/1000: episode: 20, duration: 0.118s, episode steps: 29, steps per second: 246, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: -0.068 [-2.385, 1.366], loss: 0.624534, mean_absolute_error: 2.375890, mean_q: 3.987817\n",
            " 332/1000: episode: 21, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.961, 1.501], loss: 0.654397, mean_absolute_error: 2.541488, mean_q: 4.297713\n",
            " 358/1000: episode: 22, duration: 0.111s, episode steps: 26, steps per second: 234, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.023 [-1.357, 1.931], loss: 0.650209, mean_absolute_error: 2.644409, mean_q: 4.513824\n",
            " 376/1000: episode: 23, duration: 0.081s, episode steps: 18, steps per second: 223, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.040 [-1.380, 1.955], loss: 0.622571, mean_absolute_error: 2.808520, mean_q: 4.836009\n",
            " 399/1000: episode: 24, duration: 0.105s, episode steps: 23, steps per second: 219, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.093 [-0.772, 1.173], loss: 0.718997, mean_absolute_error: 2.982471, mean_q: 5.154334\n",
            " 434/1000: episode: 25, duration: 0.144s, episode steps: 35, steps per second: 243, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.010 [-1.189, 1.720], loss: 0.753610, mean_absolute_error: 3.175357, mean_q: 5.544283\n",
            " 460/1000: episode: 26, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.083 [-0.795, 1.427], loss: 0.793737, mean_absolute_error: 3.363265, mean_q: 5.943799\n",
            " 498/1000: episode: 27, duration: 0.162s, episode steps: 38, steps per second: 234, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.070 [-1.172, 2.280], loss: 0.776866, mean_absolute_error: 3.569416, mean_q: 6.404249\n",
            " 519/1000: episode: 28, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.070 [-1.319, 2.122], loss: 0.930417, mean_absolute_error: 3.789366, mean_q: 6.801360\n",
            " 542/1000: episode: 29, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.058 [-1.153, 1.824], loss: 0.984615, mean_absolute_error: 3.948045, mean_q: 7.098032\n",
            " 559/1000: episode: 30, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.085 [-0.773, 1.396], loss: 0.986039, mean_absolute_error: 4.072219, mean_q: 7.358128\n",
            " 594/1000: episode: 31, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.043 [-1.273, 0.743], loss: 1.226107, mean_absolute_error: 4.265244, mean_q: 7.641514\n",
            " 629/1000: episode: 32, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.001 [-1.505, 0.975], loss: 1.163749, mean_absolute_error: 4.469711, mean_q: 8.037677\n",
            " 649/1000: episode: 33, duration: 0.095s, episode steps: 20, steps per second: 211, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.097 [-2.055, 1.150], loss: 1.280966, mean_absolute_error: 4.644946, mean_q: 8.343149\n",
            " 669/1000: episode: 34, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.118 [-1.241, 0.400], loss: 1.286946, mean_absolute_error: 4.765272, mean_q: 8.627992\n",
            " 688/1000: episode: 35, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.064 [-2.247, 1.378], loss: 1.555460, mean_absolute_error: 4.936679, mean_q: 8.918084\n",
            " 703/1000: episode: 36, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.091 [-1.491, 0.947], loss: 1.758839, mean_absolute_error: 5.075446, mean_q: 9.158071\n",
            " 712/1000: episode: 37, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.146, 1.924], loss: 1.387676, mean_absolute_error: 5.104910, mean_q: 9.304711\n",
            " 727/1000: episode: 38, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.103 [-1.367, 0.789], loss: 2.176070, mean_absolute_error: 5.249438, mean_q: 9.423405\n",
            " 748/1000: episode: 39, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.093 [-0.816, 1.187], loss: 2.436213, mean_absolute_error: 5.348084, mean_q: 9.523062\n",
            " 763/1000: episode: 40, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.084 [-2.405, 1.520], loss: 2.367352, mean_absolute_error: 5.409166, mean_q: 9.622131\n",
            " 774/1000: episode: 41, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.106 [-1.864, 1.154], loss: 2.531901, mean_absolute_error: 5.469387, mean_q: 9.703424\n",
            " 793/1000: episode: 42, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.103 [-1.952, 0.961], loss: 2.764987, mean_absolute_error: 5.532896, mean_q: 9.812960\n",
            " 811/1000: episode: 43, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.065 [-1.958, 1.158], loss: 2.796758, mean_absolute_error: 5.588151, mean_q: 9.929017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 831/1000: episode: 44, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.050 [-1.920, 2.949], loss: 2.684561, mean_absolute_error: 5.639667, mean_q: 10.083839\n",
            " 842/1000: episode: 45, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.156 [-1.718, 2.874], loss: 3.352817, mean_absolute_error: 5.766298, mean_q: 10.294446\n",
            " 871/1000: episode: 46, duration: 0.136s, episode steps: 29, steps per second: 213, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.086 [-1.504, 0.654], loss: 2.997024, mean_absolute_error: 5.808767, mean_q: 10.391899\n",
            " 883/1000: episode: 47, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.129 [-1.154, 2.027], loss: 3.217625, mean_absolute_error: 5.896345, mean_q: 10.492364\n",
            " 923/1000: episode: 48, duration: 0.186s, episode steps: 40, steps per second: 216, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.718, 1.205], loss: 2.753647, mean_absolute_error: 5.961963, mean_q: 10.717515\n",
            " 957/1000: episode: 49, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.631, 1.132], loss: 2.640208, mean_absolute_error: 6.178331, mean_q: 11.249049\n",
            " 972/1000: episode: 50, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-1.124, 0.581], loss: 2.949659, mean_absolute_error: 6.342363, mean_q: 11.560306\n",
            " 999/1000: episode: 51, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.107 [-1.664, 0.581], loss: 2.091800, mean_absolute_error: 6.289289, mean_q: 11.702537\n",
            "done, took 4.801 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "4ea4c0cd-664d-424e-d749-79afb275ac9e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527588391649,
          "user_tz": -60,
          "elapsed": 551,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 53.000, steps: 53\n",
            "Episode 2: reward: 84.000, steps: 84\n",
            "Episode 3: reward: 29.000, steps: 29\n",
            "Episode 4: reward: 115.000, steps: 115\n",
            "Episode 5: reward: 39.000, steps: 39\n",
            "mean: 64.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}