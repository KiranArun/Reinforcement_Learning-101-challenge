{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "43f92385-9dd0-4a20-b9b0-8f320b3f8769",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527589999978,
          "user_tz": -60,
          "elapsed": 79722,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Reinforcement_Learning-101-challenge': No such file or directory\n",
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 79, done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 79 (delta 41), reused 50 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n",
            "Extracting templates from packages: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "77805124-62c9-4eae-9e14-4b28c115b621",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590016784,
          "user_tz": -60,
          "elapsed": 13587,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "import os\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "# hack line below to get vnc to start as ipython subprocess\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
            "XWindow Link: https://4b6c6a13.ngrok.io\n",
            "Full control one-time password: 62618399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97de99f1-8f5c-4c5a-d43c-983af03e5795",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590061964,
          "user_tz": -60,
          "elapsed": 2302,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8eaabc4-06d9-4865-a39d-9ea67af40980",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590063145,
          "user_tz": -60,
          "elapsed": 583,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c54e1141-29bf-43ea-b3fc-5498002e5588",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590063753,
          "user_tz": -60,
          "elapsed": 556,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1128
        },
        "outputId": "79e0883a-acf1-4ee6-be62-5038f133fe72",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590072477,
          "user_tz": -60,
          "elapsed": 5302,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  12/1000: episode: 1, duration: 0.048s, episode steps: 12, steps per second: 251, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.123 [-1.985, 3.104], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  25/1000: episode: 2, duration: 0.009s, episode steps: 13, steps per second: 1429, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.077 [0.000, 1.000], mean observation: 0.104 [-2.166, 3.291], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  42/1000: episode: 3, duration: 0.011s, episode steps: 17, steps per second: 1482, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.052 [-2.212, 1.415], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  56/1000: episode: 4, duration: 0.010s, episode steps: 14, steps per second: 1369, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.078 [-1.233, 2.063], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  72/1000: episode: 5, duration: 0.011s, episode steps: 16, steps per second: 1497, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.092 [-2.664, 1.577], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  90/1000: episode: 6, duration: 0.012s, episode steps: 18, steps per second: 1488, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.067 [-1.230, 2.046], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 106/1000: episode: 7, duration: 0.295s, episode steps: 16, steps per second: 54, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.102 [-1.616, 2.727], loss: 0.783640, mean_absolute_error: 1.020962, mean_q: 0.222761\n",
            " 118/1000: episode: 8, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.124 [-1.970, 3.086], loss: 0.590688, mean_absolute_error: 0.919188, mean_q: 0.537239\n",
            " 126/1000: episode: 9, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.559, 1.562], loss: 0.488982, mean_absolute_error: 0.870212, mean_q: 0.807567\n",
            " 136/1000: episode: 10, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.960, 2.999], loss: 0.503583, mean_absolute_error: 0.868568, mean_q: 1.062042\n",
            " 150/1000: episode: 11, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.115 [-1.927, 3.028], loss: 0.484831, mean_absolute_error: 0.894686, mean_q: 1.326054\n",
            " 174/1000: episode: 12, duration: 0.112s, episode steps: 24, steps per second: 214, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.070 [-2.727, 1.610], loss: 0.534957, mean_absolute_error: 1.007602, mean_q: 1.718172\n",
            " 191/1000: episode: 13, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.060 [-1.407, 2.314], loss: 0.531844, mean_absolute_error: 1.124236, mean_q: 2.078793\n",
            " 205/1000: episode: 14, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.090 [-2.985, 1.954], loss: 0.530605, mean_absolute_error: 1.250448, mean_q: 2.348599\n",
            " 217/1000: episode: 15, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.113, 1.197], loss: 0.601874, mean_absolute_error: 1.358678, mean_q: 2.629006\n",
            " 229/1000: episode: 16, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.118 [-1.561, 2.497], loss: 0.625010, mean_absolute_error: 1.494054, mean_q: 2.833833\n",
            " 238/1000: episode: 17, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.749, 2.763], loss: 0.666327, mean_absolute_error: 1.593599, mean_q: 2.997465\n",
            " 258/1000: episode: 18, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.035 [-2.950, 1.965], loss: 0.692631, mean_absolute_error: 1.738145, mean_q: 3.208179\n",
            " 272/1000: episode: 19, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.067 [-2.464, 1.613], loss: 0.689735, mean_absolute_error: 1.922808, mean_q: 3.450000\n",
            " 283/1000: episode: 20, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.228, 1.359], loss: 0.740290, mean_absolute_error: 2.075190, mean_q: 3.652790\n",
            " 296/1000: episode: 21, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.095 [-1.019, 1.812], loss: 0.776033, mean_absolute_error: 2.225188, mean_q: 3.831422\n",
            " 307/1000: episode: 22, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.126 [-1.774, 1.007], loss: 0.762235, mean_absolute_error: 2.333244, mean_q: 3.964305\n",
            " 331/1000: episode: 23, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.787, 1.256], loss: 0.673028, mean_absolute_error: 2.467041, mean_q: 4.157083\n",
            " 344/1000: episode: 24, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.118 [-1.180, 2.001], loss: 0.688532, mean_absolute_error: 2.599706, mean_q: 4.352271\n",
            " 376/1000: episode: 25, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.749, 1.051], loss: 0.677701, mean_absolute_error: 2.738595, mean_q: 4.596821\n",
            " 392/1000: episode: 26, duration: 0.077s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.098 [-0.767, 1.306], loss: 0.726894, mean_absolute_error: 2.902111, mean_q: 4.915847\n",
            " 407/1000: episode: 27, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.074 [-0.839, 1.432], loss: 0.751935, mean_absolute_error: 2.993330, mean_q: 5.101554\n",
            " 427/1000: episode: 28, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.099 [-1.520, 2.617], loss: 0.772105, mean_absolute_error: 3.078517, mean_q: 5.300521\n",
            " 439/1000: episode: 29, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.130 [-1.150, 2.014], loss: 0.772675, mean_absolute_error: 3.167646, mean_q: 5.508535\n",
            " 474/1000: episode: 30, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.012 [-1.350, 2.038], loss: 0.835662, mean_absolute_error: 3.310703, mean_q: 5.839869\n",
            " 502/1000: episode: 31, duration: 0.131s, episode steps: 28, steps per second: 215, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: -0.003 [-1.353, 1.846], loss: 0.904565, mean_absolute_error: 3.531455, mean_q: 6.260841\n",
            " 520/1000: episode: 32, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.068 [-0.991, 1.657], loss: 0.972055, mean_absolute_error: 3.678037, mean_q: 6.552564\n",
            " 564/1000: episode: 33, duration: 0.216s, episode steps: 44, steps per second: 203, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.057 [-0.648, 1.407], loss: 0.987207, mean_absolute_error: 3.866263, mean_q: 6.951448\n",
            " 581/1000: episode: 34, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.069 [-1.165, 1.849], loss: 0.916388, mean_absolute_error: 4.050124, mean_q: 7.341849\n",
            " 595/1000: episode: 35, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.102 [-1.662, 0.952], loss: 0.939964, mean_absolute_error: 4.156754, mean_q: 7.571988\n",
            " 632/1000: episode: 36, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.061 [-1.044, 0.599], loss: 1.095426, mean_absolute_error: 4.380940, mean_q: 7.969384\n",
            " 645/1000: episode: 37, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.112 [-1.936, 1.133], loss: 1.274936, mean_absolute_error: 4.626459, mean_q: 8.379553\n",
            " 656/1000: episode: 38, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.091 [-1.616, 1.033], loss: 1.202420, mean_absolute_error: 4.723839, mean_q: 8.597175\n",
            " 669/1000: episode: 39, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.748, 1.761], loss: 1.629738, mean_absolute_error: 4.887057, mean_q: 8.858362\n",
            " 678/1000: episode: 40, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.740, 2.856], loss: 1.785151, mean_absolute_error: 4.977392, mean_q: 9.096073\n",
            " 689/1000: episode: 41, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.117 [-1.765, 2.775], loss: 2.280664, mean_absolute_error: 5.061174, mean_q: 9.224603\n",
            " 701/1000: episode: 42, duration: 0.058s, episode steps: 12, steps per second: 207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.126 [-2.112, 1.167], loss: 2.528689, mean_absolute_error: 5.153972, mean_q: 9.382786\n",
            " 710/1000: episode: 43, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.791, 1.766], loss: 2.757689, mean_absolute_error: 5.252549, mean_q: 9.560025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 725/1000: episode: 44, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.081 [-2.861, 1.900], loss: 2.985959, mean_absolute_error: 5.350154, mean_q: 9.681129\n",
            " 735/1000: episode: 45, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.032, 1.201], loss: 3.331762, mean_absolute_error: 5.437999, mean_q: 9.862997\n",
            " 761/1000: episode: 46, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.001 [-2.790, 1.968], loss: 3.576557, mean_absolute_error: 5.532123, mean_q: 9.906427\n",
            " 774/1000: episode: 47, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.079 [-1.437, 1.009], loss: 3.517100, mean_absolute_error: 5.594073, mean_q: 10.007885\n",
            " 784/1000: episode: 48, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.132 [-1.869, 1.145], loss: 3.494470, mean_absolute_error: 5.635807, mean_q: 10.062669\n",
            " 795/1000: episode: 49, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-1.740, 1.029], loss: 3.146243, mean_absolute_error: 5.626200, mean_q: 10.126801\n",
            " 809/1000: episode: 50, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.086 [-2.602, 1.602], loss: 3.495021, mean_absolute_error: 5.713985, mean_q: 10.160970\n",
            " 821/1000: episode: 51, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.110 [-1.934, 1.157], loss: 3.154177, mean_absolute_error: 5.650771, mean_q: 10.105979\n",
            " 834/1000: episode: 52, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.442, 1.516], loss: 2.907086, mean_absolute_error: 5.538014, mean_q: 9.942110\n",
            " 852/1000: episode: 53, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.065 [-2.426, 1.540], loss: 2.820951, mean_absolute_error: 5.513688, mean_q: 9.915175\n",
            " 871/1000: episode: 54, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.070 [-2.020, 1.226], loss: 2.612700, mean_absolute_error: 5.490850, mean_q: 9.943989\n",
            " 887/1000: episode: 55, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.058 [-1.416, 0.813], loss: 2.440873, mean_absolute_error: 5.486341, mean_q: 9.953014\n",
            " 918/1000: episode: 56, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.086 [-1.373, 0.726], loss: 2.496707, mean_absolute_error: 5.532282, mean_q: 9.982919\n",
            " 939/1000: episode: 57, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.081 [-1.418, 0.586], loss: 2.117306, mean_absolute_error: 5.624024, mean_q: 10.259053\n",
            " 966/1000: episode: 58, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.061 [-1.516, 0.810], loss: 1.951272, mean_absolute_error: 5.764865, mean_q: 10.575111\n",
            "done, took 5.039 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "e096172e-721d-472c-b8e6-42e3d71365fb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590075768,
          "user_tz": -60,
          "elapsed": 2066,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 49.000, steps: 49\n",
            "Episode 2: reward: 93.000, steps: 93\n",
            "Episode 3: reward: 46.000, steps: 46\n",
            "Episode 4: reward: 48.000, steps: 48\n",
            "Episode 5: reward: 43.000, steps: 43\n",
            "Episode 6: reward: 101.000, steps: 101\n",
            "Episode 7: reward: 97.000, steps: 97\n",
            "Episode 8: reward: 52.000, steps: 52\n",
            "Episode 9: reward: 52.000, steps: 52\n",
            "Episode 10: reward: 55.000, steps: 55\n",
            "Episode 11: reward: 45.000, steps: 45\n",
            "Episode 12: reward: 51.000, steps: 51\n",
            "Episode 13: reward: 78.000, steps: 78\n",
            "Episode 14: reward: 78.000, steps: 78\n",
            "Episode 15: reward: 67.000, steps: 67\n",
            "Episode 16: reward: 45.000, steps: 45\n",
            "Episode 17: reward: 90.000, steps: 90\n",
            "Episode 18: reward: 57.000, steps: 57\n",
            "Episode 19: reward: 77.000, steps: 77\n",
            "Episode 20: reward: 59.000, steps: 59\n",
            "Episode 21: reward: 63.000, steps: 63\n",
            "Episode 22: reward: 43.000, steps: 43\n",
            "Episode 23: reward: 78.000, steps: 78\n",
            "Episode 24: reward: 45.000, steps: 45\n",
            "Episode 25: reward: 44.000, steps: 44\n",
            "Episode 26: reward: 59.000, steps: 59\n",
            "Episode 27: reward: 58.000, steps: 58\n",
            "Episode 28: reward: 89.000, steps: 89\n",
            "Episode 29: reward: 90.000, steps: 90\n",
            "Episode 30: reward: 75.000, steps: 75\n",
            "Episode 31: reward: 104.000, steps: 104\n",
            "Episode 32: reward: 95.000, steps: 95\n",
            "Episode 33: reward: 53.000, steps: 53\n",
            "Episode 34: reward: 85.000, steps: 85\n",
            "Episode 35: reward: 53.000, steps: 53\n",
            "Episode 36: reward: 58.000, steps: 58\n",
            "Episode 37: reward: 56.000, steps: 56\n",
            "Episode 38: reward: 77.000, steps: 77\n",
            "Episode 39: reward: 83.000, steps: 83\n",
            "Episode 40: reward: 56.000, steps: 56\n",
            "Episode 41: reward: 86.000, steps: 86\n",
            "Episode 42: reward: 63.000, steps: 63\n",
            "Episode 43: reward: 51.000, steps: 51\n",
            "Episode 44: reward: 52.000, steps: 52\n",
            "Episode 45: reward: 76.000, steps: 76\n",
            "Episode 46: reward: 48.000, steps: 48\n",
            "Episode 47: reward: 48.000, steps: 48\n",
            "Episode 48: reward: 46.000, steps: 46\n",
            "Episode 49: reward: 72.000, steps: 72\n",
            "Episode 50: reward: 54.000, steps: 54\n",
            "mean: 64.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model to file\n",
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}