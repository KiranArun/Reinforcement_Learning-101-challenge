{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0a594718-442e-4065-b092-2933cc859227",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594296908,
          "user_tz": -60,
          "elapsed": 60264,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# setup\n",
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Reinforcement_Learning-101-challenge': No such file or directory\n",
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 95, done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 95 (delta 51), reused 60 (delta 24), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n",
            "Extracting templates from packages: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f1b05fe8-a6ee-4a05-97b2-be5658b7add7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594372773,
          "user_tz": -60,
          "elapsed": 13440,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "import os\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "# hack line below to get vnc to start as ipython subprocess\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
            "XWindow Link: http://a22ec943.ngrok.io\n",
            "Full control one-time password: 42590875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8989ccd3-5926-4af6-e872-1e23e0452a79",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594480606,
          "user_tz": -60,
          "elapsed": 7965,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a07e9840-e3a6-44c0-85cd-9ed481320707",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594482048,
          "user_tz": -60,
          "elapsed": 535,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "159fb612-3a28-4e62-b824-5a6d437dcadf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594482627,
          "user_tz": -60,
          "elapsed": 484,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=150,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkHWScERfz0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1092
        },
        "outputId": "d3b69830-5283-463d-fb0b-20f6530484bc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594496339,
          "user_tz": -60,
          "elapsed": 9351,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "   8/1000: episode: 1, duration: 0.258s, episode steps: 8, steps per second: 31, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.156 [-1.190, 2.016], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  18/1000: episode: 2, duration: 0.017s, episode steps: 10, steps per second: 586, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.765, 1.795], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  40/1000: episode: 3, duration: 0.038s, episode steps: 22, steps per second: 575, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.037 [-1.209, 1.984], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  55/1000: episode: 4, duration: 0.021s, episode steps: 15, steps per second: 700, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-0.768, 1.292], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  71/1000: episode: 5, duration: 0.024s, episode steps: 16, steps per second: 670, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.082 [-1.562, 2.622], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  89/1000: episode: 6, duration: 0.026s, episode steps: 18, steps per second: 696, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.064 [-2.445, 1.600], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 102/1000: episode: 7, duration: 0.019s, episode steps: 13, steps per second: 688, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.130 [-0.750, 1.320], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 144/1000: episode: 8, duration: 0.072s, episode steps: 42, steps per second: 587, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.039 [-1.477, 1.554], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 157/1000: episode: 9, duration: 0.387s, episode steps: 13, steps per second: 34, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.106 [-1.384, 2.311], loss: 0.643156, mean_absolute_error: 0.639956, mean_q: 0.145584\n",
            " 168/1000: episode: 10, duration: 0.108s, episode steps: 11, steps per second: 102, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-1.878, 1.192], loss: 0.563114, mean_absolute_error: 0.601677, mean_q: 0.214261\n",
            " 182/1000: episode: 11, duration: 0.132s, episode steps: 14, steps per second: 106, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.100 [-2.366, 1.528], loss: 0.517539, mean_absolute_error: 0.592875, mean_q: 0.298424\n",
            " 216/1000: episode: 12, duration: 0.302s, episode steps: 34, steps per second: 113, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.022 [-1.135, 1.280], loss: 0.448996, mean_absolute_error: 0.591779, mean_q: 0.415035\n",
            " 226/1000: episode: 13, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.202, 2.028], loss: 0.386559, mean_absolute_error: 0.589441, mean_q: 0.511155\n",
            " 243/1000: episode: 14, duration: 0.165s, episode steps: 17, steps per second: 103, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.087 [-1.780, 0.944], loss: 0.388699, mean_absolute_error: 0.630354, mean_q: 0.579069\n",
            " 261/1000: episode: 15, duration: 0.167s, episode steps: 18, steps per second: 108, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.093 [-0.832, 1.693], loss: 0.354798, mean_absolute_error: 0.637186, mean_q: 0.649489\n",
            " 282/1000: episode: 16, duration: 0.198s, episode steps: 21, steps per second: 106, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.063 [-2.260, 1.327], loss: 0.342593, mean_absolute_error: 0.683070, mean_q: 0.746947\n",
            " 295/1000: episode: 17, duration: 0.120s, episode steps: 13, steps per second: 108, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.115 [-2.490, 1.602], loss: 0.311668, mean_absolute_error: 0.680576, mean_q: 0.808643\n",
            " 306/1000: episode: 18, duration: 0.112s, episode steps: 11, steps per second: 98, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.098 [-1.417, 2.235], loss: 0.330285, mean_absolute_error: 0.721427, mean_q: 0.851288\n",
            " 320/1000: episode: 19, duration: 0.125s, episode steps: 14, steps per second: 112, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.099 [-1.184, 1.977], loss: 0.309169, mean_absolute_error: 0.732867, mean_q: 0.912156\n",
            " 331/1000: episode: 20, duration: 0.115s, episode steps: 11, steps per second: 96, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.106 [-1.001, 1.695], loss: 0.316737, mean_absolute_error: 0.753962, mean_q: 0.983587\n",
            " 348/1000: episode: 21, duration: 0.158s, episode steps: 17, steps per second: 108, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.088 [-1.338, 2.355], loss: 0.314763, mean_absolute_error: 0.799589, mean_q: 1.008643\n",
            " 361/1000: episode: 22, duration: 0.136s, episode steps: 13, steps per second: 96, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.119 [-1.801, 0.966], loss: 0.313258, mean_absolute_error: 0.827706, mean_q: 1.078172\n",
            " 378/1000: episode: 23, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.082 [-1.337, 2.076], loss: 0.289968, mean_absolute_error: 0.836907, mean_q: 1.125681\n",
            " 392/1000: episode: 24, duration: 0.139s, episode steps: 14, steps per second: 101, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.100 [-0.978, 1.680], loss: 0.303688, mean_absolute_error: 0.877592, mean_q: 1.211534\n",
            " 409/1000: episode: 25, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.098 [-0.955, 1.682], loss: 0.313760, mean_absolute_error: 0.924889, mean_q: 1.265628\n",
            " 422/1000: episode: 26, duration: 0.137s, episode steps: 13, steps per second: 95, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.085 [-2.168, 1.344], loss: 0.312313, mean_absolute_error: 0.961750, mean_q: 1.348026\n",
            " 445/1000: episode: 27, duration: 0.203s, episode steps: 23, steps per second: 113, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.075 [-1.334, 2.344], loss: 0.306954, mean_absolute_error: 0.988154, mean_q: 1.385146\n",
            " 455/1000: episode: 28, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.606, 2.603], loss: 0.307731, mean_absolute_error: 1.013312, mean_q: 1.437744\n",
            " 465/1000: episode: 29, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.165, 2.032], loss: 0.303054, mean_absolute_error: 1.041533, mean_q: 1.511082\n",
            " 474/1000: episode: 30, duration: 0.111s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.771, 1.802], loss: 0.315433, mean_absolute_error: 1.070985, mean_q: 1.523721\n",
            " 491/1000: episode: 31, duration: 0.158s, episode steps: 17, steps per second: 107, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.085 [-1.367, 2.286], loss: 0.307135, mean_absolute_error: 1.085575, mean_q: 1.571322\n",
            " 530/1000: episode: 32, duration: 0.360s, episode steps: 39, steps per second: 108, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: -0.005 [-1.558, 1.137], loss: 0.306929, mean_absolute_error: 1.137379, mean_q: 1.672490\n",
            " 542/1000: episode: 33, duration: 0.109s, episode steps: 12, steps per second: 110, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.147 [-1.140, 2.152], loss: 0.309065, mean_absolute_error: 1.170025, mean_q: 1.733601\n",
            " 554/1000: episode: 34, duration: 0.116s, episode steps: 12, steps per second: 104, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-2.069, 1.326], loss: 0.314354, mean_absolute_error: 1.202476, mean_q: 1.802649\n",
            " 566/1000: episode: 35, duration: 0.109s, episode steps: 12, steps per second: 111, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.103 [-1.614, 2.493], loss: 0.295011, mean_absolute_error: 1.208352, mean_q: 1.818763\n",
            " 602/1000: episode: 36, duration: 0.337s, episode steps: 36, steps per second: 107, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.009 [-1.603, 2.374], loss: 0.321048, mean_absolute_error: 1.266980, mean_q: 1.936505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 626/1000: episode: 37, duration: 0.216s, episode steps: 24, steps per second: 111, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.055 [-0.959, 1.492], loss: 0.332434, mean_absolute_error: 1.328621, mean_q: 2.049362\n",
            " 638/1000: episode: 38, duration: 0.107s, episode steps: 12, steps per second: 112, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-1.524, 0.841], loss: 0.346361, mean_absolute_error: 1.362377, mean_q: 2.107700\n",
            " 651/1000: episode: 39, duration: 0.124s, episode steps: 13, steps per second: 105, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.111 [-1.333, 2.253], loss: 0.345310, mean_absolute_error: 1.392566, mean_q: 2.159372\n",
            " 667/1000: episode: 40, duration: 0.151s, episode steps: 16, steps per second: 106, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.094 [-0.749, 1.391], loss: 0.351447, mean_absolute_error: 1.424789, mean_q: 2.211446\n",
            " 683/1000: episode: 41, duration: 0.147s, episode steps: 16, steps per second: 109, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.097 [-1.030, 1.840], loss: 0.363422, mean_absolute_error: 1.461157, mean_q: 2.287103\n",
            " 696/1000: episode: 42, duration: 0.128s, episode steps: 13, steps per second: 101, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.099 [-1.405, 2.261], loss: 0.354724, mean_absolute_error: 1.485964, mean_q: 2.337045\n",
            " 718/1000: episode: 43, duration: 0.208s, episode steps: 22, steps per second: 106, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.072 [-0.625, 1.024], loss: 0.374270, mean_absolute_error: 1.514428, mean_q: 2.390140\n",
            " 746/1000: episode: 44, duration: 0.264s, episode steps: 28, steps per second: 106, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.074 [-1.212, 2.331], loss: 0.360824, mean_absolute_error: 1.567041, mean_q: 2.483093\n",
            " 769/1000: episode: 45, duration: 0.232s, episode steps: 23, steps per second: 99, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.067 [-0.965, 1.710], loss: 0.359081, mean_absolute_error: 1.600010, mean_q: 2.572602\n",
            " 786/1000: episode: 46, duration: 0.152s, episode steps: 17, steps per second: 112, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.104 [-1.320, 0.565], loss: 0.388633, mean_absolute_error: 1.644883, mean_q: 2.646792\n",
            " 802/1000: episode: 47, duration: 0.163s, episode steps: 16, steps per second: 98, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.090 [-0.577, 1.289], loss: 0.362599, mean_absolute_error: 1.659374, mean_q: 2.682259\n",
            " 820/1000: episode: 48, duration: 0.170s, episode steps: 18, steps per second: 106, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.082 [-1.563, 2.632], loss: 0.386998, mean_absolute_error: 1.705526, mean_q: 2.762481\n",
            " 833/1000: episode: 49, duration: 0.128s, episode steps: 13, steps per second: 102, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.116 [-1.749, 2.772], loss: 0.413652, mean_absolute_error: 1.744745, mean_q: 2.809988\n",
            " 849/1000: episode: 50, duration: 0.150s, episode steps: 16, steps per second: 107, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.091 [-2.047, 1.210], loss: 0.435986, mean_absolute_error: 1.775994, mean_q: 2.865255\n",
            " 878/1000: episode: 51, duration: 0.272s, episode steps: 29, steps per second: 107, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.135 [-0.585, 1.409], loss: 0.431450, mean_absolute_error: 1.811274, mean_q: 2.950046\n",
            " 887/1000: episode: 52, duration: 0.088s, episode steps: 9, steps per second: 102, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.184 [-1.523, 2.550], loss: 0.429908, mean_absolute_error: 1.846800, mean_q: 3.016511\n",
            " 902/1000: episode: 53, duration: 0.142s, episode steps: 15, steps per second: 106, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.088 [-1.477, 0.829], loss: 0.455855, mean_absolute_error: 1.877938, mean_q: 3.078376\n",
            " 937/1000: episode: 54, duration: 0.318s, episode steps: 35, steps per second: 110, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.047 [-1.270, 0.641], loss: 0.444390, mean_absolute_error: 1.914829, mean_q: 3.139123\n",
            " 959/1000: episode: 55, duration: 0.198s, episode steps: 22, steps per second: 111, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.743, 1.178], loss: 0.437849, mean_absolute_error: 1.954975, mean_q: 3.217690\n",
            " 985/1000: episode: 56, duration: 0.251s, episode steps: 26, steps per second: 104, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.086 [-0.967, 1.941], loss: 0.414860, mean_absolute_error: 1.993641, mean_q: 3.311802\n",
            "done, took 8.856 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2V7EmPbCf34v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "ab8bb9db-709e-45e2-a388-0fb6b7ac3368",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527594497710,
          "user_tz": -60,
          "elapsed": 1343,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 10.000, steps: 10\n",
            "Episode 3: reward: 10.000, steps: 10\n",
            "Episode 4: reward: 8.000, steps: 8\n",
            "Episode 5: reward: 10.000, steps: 10\n",
            "Episode 6: reward: 10.000, steps: 10\n",
            "Episode 7: reward: 10.000, steps: 10\n",
            "Episode 8: reward: 9.000, steps: 9\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 9.000, steps: 9\n",
            "Episode 11: reward: 10.000, steps: 10\n",
            "Episode 12: reward: 11.000, steps: 11\n",
            "Episode 13: reward: 9.000, steps: 9\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 10.000, steps: 10\n",
            "Episode 16: reward: 9.000, steps: 9\n",
            "Episode 17: reward: 10.000, steps: 10\n",
            "Episode 18: reward: 9.000, steps: 9\n",
            "Episode 19: reward: 10.000, steps: 10\n",
            "Episode 20: reward: 9.000, steps: 9\n",
            "Episode 21: reward: 9.000, steps: 9\n",
            "Episode 22: reward: 10.000, steps: 10\n",
            "Episode 23: reward: 10.000, steps: 10\n",
            "Episode 24: reward: 10.000, steps: 10\n",
            "Episode 25: reward: 8.000, steps: 8\n",
            "Episode 26: reward: 10.000, steps: 10\n",
            "Episode 27: reward: 8.000, steps: 8\n",
            "Episode 28: reward: 10.000, steps: 10\n",
            "Episode 29: reward: 8.000, steps: 8\n",
            "Episode 30: reward: 10.000, steps: 10\n",
            "Episode 31: reward: 9.000, steps: 9\n",
            "Episode 32: reward: 8.000, steps: 8\n",
            "Episode 33: reward: 8.000, steps: 8\n",
            "Episode 34: reward: 9.000, steps: 9\n",
            "Episode 35: reward: 9.000, steps: 9\n",
            "Episode 36: reward: 9.000, steps: 9\n",
            "Episode 37: reward: 9.000, steps: 9\n",
            "Episode 38: reward: 10.000, steps: 10\n",
            "Episode 39: reward: 10.000, steps: 10\n",
            "Episode 40: reward: 9.000, steps: 9\n",
            "Episode 41: reward: 11.000, steps: 11\n",
            "Episode 42: reward: 9.000, steps: 9\n",
            "Episode 43: reward: 10.000, steps: 10\n",
            "Episode 44: reward: 11.000, steps: 11\n",
            "Episode 45: reward: 9.000, steps: 9\n",
            "Episode 46: reward: 10.000, steps: 10\n",
            "Episode 47: reward: 10.000, steps: 10\n",
            "Episode 48: reward: 11.000, steps: 11\n",
            "Episode 49: reward: 9.000, steps: 9\n",
            "Episode 50: reward: 10.000, steps: 10\n",
            "mean: 9.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tgFVcwMftmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Displaying test games\n",
        "\n",
        "The below cell will save just the neural net with its weights.\n",
        "\n",
        "To run the test with display:\n",
        "\n",
        "1. Go to the XWindow link, and login if you haven't already\n",
        "2. In the TightVNC terminal, type and run:\n",
        "   \n",
        "    `run <number_repeats>`\n",
        "    - replace `<number_repeats>` with however many games you want it to play (or leave it blank to run once)\n"
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model to file\n",
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}