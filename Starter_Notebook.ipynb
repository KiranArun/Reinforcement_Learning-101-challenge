{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "0cbf77d8-e2ca-4092-c3ec-b1d8c3b45898",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270416461,
          "user_tz": -60,
          "elapsed": 74933,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# setup\n",
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Reinforcement_Learning-101-challenge': No such file or directory\n",
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 99, done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 99 (delta 53), reused 65 (delta 27), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n",
            "Extracting templates from packages: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "56ca8905-7823-4f77-dd62-b1b0815fd647",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270430172,
          "user_tz": -60,
          "elapsed": 13655,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "import os\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "# hack line below to get vnc to start as ipython subprocess\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
            "XWindow Link: https://41d547c2.ngrok.io\n",
            "Full control one-time password: 33630616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000bd038-8aa0-4fc3-ccdb-1f82fde5f406",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270439102,
          "user_tz": -60,
          "elapsed": 1430,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e955e8e1-7399-41fc-df3d-1a3db61856c4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270440697,
          "user_tz": -60,
          "elapsed": 779,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "97b02b65-d8e9-4cde-a706-c45ae20d527f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270441333,
          "user_tz": -60,
          "elapsed": 488,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "mem = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "pol = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=mem,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=pol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkHWScERfz0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "a948d0b9-258c-47e5-a358-e2cf366e9b0d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270449021,
          "user_tz": -60,
          "elapsed": 4001,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  12/1000: episode: 1, duration: 0.044s, episode steps: 12, steps per second: 271, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.092 [-2.058, 1.223], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  26/1000: episode: 2, duration: 0.010s, episode steps: 14, steps per second: 1423, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.118 [-1.531, 0.757], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  44/1000: episode: 3, duration: 0.012s, episode steps: 18, steps per second: 1465, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.052 [-1.563, 0.955], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  63/1000: episode: 4, duration: 0.013s, episode steps: 19, steps per second: 1409, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.082 [-1.856, 0.982], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  79/1000: episode: 5, duration: 0.011s, episode steps: 16, steps per second: 1456, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.098 [-1.205, 2.093], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  95/1000: episode: 6, duration: 0.011s, episode steps: 16, steps per second: 1457, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-0.916, 0.396], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 112/1000: episode: 7, duration: 0.297s, episode steps: 17, steps per second: 57, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.108 [-2.083, 1.158], loss: 0.510949, mean_absolute_error: 0.666718, mean_q: 0.261904\n",
            " 169/1000: episode: 8, duration: 0.192s, episode steps: 57, steps per second: 296, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.152 [-1.445, 1.698], loss: 0.388708, mean_absolute_error: 0.660545, mean_q: 0.443437\n",
            " 186/1000: episode: 9, duration: 0.067s, episode steps: 17, steps per second: 254, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.076 [-1.325, 2.230], loss: 0.337490, mean_absolute_error: 0.663771, mean_q: 0.519322\n",
            " 218/1000: episode: 10, duration: 0.106s, episode steps: 32, steps per second: 301, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.143 [-1.384, 0.581], loss: 0.323697, mean_absolute_error: 0.689117, mean_q: 0.589272\n",
            " 236/1000: episode: 11, duration: 0.059s, episode steps: 18, steps per second: 306, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.086 [-2.264, 1.348], loss: 0.317441, mean_absolute_error: 0.734650, mean_q: 0.736727\n",
            " 252/1000: episode: 12, duration: 0.075s, episode steps: 16, steps per second: 213, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.083 [-1.938, 1.189], loss: 0.298557, mean_absolute_error: 0.762231, mean_q: 0.850637\n",
            " 282/1000: episode: 13, duration: 0.119s, episode steps: 30, steps per second: 251, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-1.413, 0.963], loss: 0.293977, mean_absolute_error: 0.810731, mean_q: 0.993466\n",
            " 293/1000: episode: 14, duration: 0.040s, episode steps: 11, steps per second: 274, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.134 [-1.745, 0.976], loss: 0.305424, mean_absolute_error: 0.846063, mean_q: 1.065843\n",
            " 307/1000: episode: 15, duration: 0.060s, episode steps: 14, steps per second: 235, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.085 [-2.149, 1.375], loss: 0.290719, mean_absolute_error: 0.864569, mean_q: 1.117568\n",
            " 331/1000: episode: 16, duration: 0.098s, episode steps: 24, steps per second: 246, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.143 [-1.581, 0.551], loss: 0.302745, mean_absolute_error: 0.928920, mean_q: 1.236873\n",
            " 343/1000: episode: 17, duration: 0.047s, episode steps: 12, steps per second: 256, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-1.952, 1.168], loss: 0.310552, mean_absolute_error: 0.977650, mean_q: 1.361853\n",
            " 355/1000: episode: 18, duration: 0.042s, episode steps: 12, steps per second: 283, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.093 [-2.077, 1.402], loss: 0.314260, mean_absolute_error: 0.984658, mean_q: 1.391472\n",
            " 381/1000: episode: 19, duration: 0.090s, episode steps: 26, steps per second: 289, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.050 [-1.829, 1.149], loss: 0.333654, mean_absolute_error: 1.043758, mean_q: 1.489609\n",
            " 393/1000: episode: 20, duration: 0.047s, episode steps: 12, steps per second: 253, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.129 [-1.346, 0.762], loss: 0.336894, mean_absolute_error: 1.097003, mean_q: 1.618849\n",
            " 413/1000: episode: 21, duration: 0.073s, episode steps: 20, steps per second: 275, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.072 [-0.652, 1.358], loss: 0.333078, mean_absolute_error: 1.132344, mean_q: 1.685304\n",
            " 423/1000: episode: 22, duration: 0.036s, episode steps: 10, steps per second: 279, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.129 [-0.956, 1.522], loss: 0.337234, mean_absolute_error: 1.121392, mean_q: 1.654749\n",
            " 447/1000: episode: 23, duration: 0.091s, episode steps: 24, steps per second: 265, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.048 [-1.599, 0.973], loss: 0.351479, mean_absolute_error: 1.184241, mean_q: 1.773126\n",
            " 469/1000: episode: 24, duration: 0.085s, episode steps: 22, steps per second: 258, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.099 [-0.779, 1.362], loss: 0.362501, mean_absolute_error: 1.216122, mean_q: 1.833188\n",
            " 504/1000: episode: 25, duration: 0.112s, episode steps: 35, steps per second: 312, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.065 [-0.609, 1.427], loss: 0.366394, mean_absolute_error: 1.245999, mean_q: 1.881897\n",
            " 557/1000: episode: 26, duration: 0.188s, episode steps: 53, steps per second: 281, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: 0.028 [-2.555, 1.930], loss: 0.377205, mean_absolute_error: 1.313177, mean_q: 2.036498\n",
            " 572/1000: episode: 27, duration: 0.051s, episode steps: 15, steps per second: 297, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-2.116, 1.353], loss: 0.374279, mean_absolute_error: 1.405763, mean_q: 2.272510\n",
            " 589/1000: episode: 28, duration: 0.059s, episode steps: 17, steps per second: 290, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.056 [-2.657, 1.771], loss: 0.369004, mean_absolute_error: 1.455129, mean_q: 2.375021\n",
            " 603/1000: episode: 29, duration: 0.054s, episode steps: 14, steps per second: 262, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.083 [-1.677, 0.992], loss: 0.432014, mean_absolute_error: 1.502515, mean_q: 2.465014\n",
            " 616/1000: episode: 30, duration: 0.052s, episode steps: 13, steps per second: 249, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.101 [-1.480, 0.787], loss: 0.493135, mean_absolute_error: 1.574843, mean_q: 2.575903\n",
            " 641/1000: episode: 31, duration: 0.082s, episode steps: 25, steps per second: 306, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-2.024, 1.014], loss: 0.419936, mean_absolute_error: 1.559842, mean_q: 2.585421\n",
            " 656/1000: episode: 32, duration: 0.050s, episode steps: 15, steps per second: 300, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.061 [-2.177, 1.366], loss: 0.445144, mean_absolute_error: 1.604091, mean_q: 2.647644\n",
            " 686/1000: episode: 33, duration: 0.110s, episode steps: 30, steps per second: 274, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.637, 1.168], loss: 0.537624, mean_absolute_error: 1.683845, mean_q: 2.773850\n",
            " 707/1000: episode: 34, duration: 0.072s, episode steps: 21, steps per second: 291, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.069 [-2.551, 1.559], loss: 0.520332, mean_absolute_error: 1.711786, mean_q: 2.819727\n",
            " 715/1000: episode: 35, duration: 0.031s, episode steps: 8, steps per second: 260, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-2.541, 1.615], loss: 0.488717, mean_absolute_error: 1.647014, mean_q: 2.755825\n",
            " 727/1000: episode: 36, duration: 0.043s, episode steps: 12, steps per second: 281, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.126 [-3.098, 1.922], loss: 0.515847, mean_absolute_error: 1.752395, mean_q: 2.953324\n",
            " 741/1000: episode: 37, duration: 0.049s, episode steps: 14, steps per second: 286, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.108 [-1.470, 0.776], loss: 0.508205, mean_absolute_error: 1.748169, mean_q: 2.966398\n",
            " 754/1000: episode: 38, duration: 0.056s, episode steps: 13, steps per second: 233, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.077 [-2.770, 1.804], loss: 0.522064, mean_absolute_error: 1.800691, mean_q: 3.089516\n",
            " 775/1000: episode: 39, duration: 0.073s, episode steps: 21, steps per second: 288, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.044 [-1.846, 1.166], loss: 0.613219, mean_absolute_error: 1.840892, mean_q: 3.140621\n",
            " 789/1000: episode: 40, duration: 0.049s, episode steps: 14, steps per second: 288, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.097 [-2.488, 1.540], loss: 0.565973, mean_absolute_error: 1.855891, mean_q: 3.237062\n",
            " 801/1000: episode: 41, duration: 0.047s, episode steps: 12, steps per second: 253, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.098 [-2.514, 1.592], loss: 0.606003, mean_absolute_error: 1.923895, mean_q: 3.354867\n",
            " 813/1000: episode: 42, duration: 0.046s, episode steps: 12, steps per second: 259, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.046, 1.196], loss: 0.603325, mean_absolute_error: 1.947946, mean_q: 3.467486\n",
            " 826/1000: episode: 43, duration: 0.056s, episode steps: 13, steps per second: 233, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.110 [-2.845, 1.762], loss: 0.729127, mean_absolute_error: 2.052627, mean_q: 3.628410\n",
            " 851/1000: episode: 44, duration: 0.088s, episode steps: 25, steps per second: 283, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.030 [-2.558, 1.726], loss: 0.746059, mean_absolute_error: 2.069809, mean_q: 3.663509\n",
            " 861/1000: episode: 45, duration: 0.038s, episode steps: 10, steps per second: 260, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-1.947, 1.184], loss: 0.906526, mean_absolute_error: 2.207309, mean_q: 3.868753\n",
            " 878/1000: episode: 46, duration: 0.074s, episode steps: 17, steps per second: 230, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.085 [-1.881, 1.041], loss: 0.793879, mean_absolute_error: 2.181568, mean_q: 3.902161\n",
            " 897/1000: episode: 47, duration: 0.074s, episode steps: 19, steps per second: 258, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.145 [-0.545, 1.035], loss: 0.931237, mean_absolute_error: 2.241428, mean_q: 3.947061\n",
            " 910/1000: episode: 48, duration: 0.045s, episode steps: 13, steps per second: 286, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.105 [-2.312, 1.357], loss: 0.872036, mean_absolute_error: 2.260308, mean_q: 4.057119\n",
            " 923/1000: episode: 49, duration: 0.045s, episode steps: 13, steps per second: 287, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.094 [-2.308, 1.411], loss: 0.983244, mean_absolute_error: 2.298121, mean_q: 4.087207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 933/1000: episode: 50, duration: 0.042s, episode steps: 10, steps per second: 236, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.256, 1.413], loss: 0.928980, mean_absolute_error: 2.326873, mean_q: 4.151675\n",
            " 944/1000: episode: 51, duration: 0.038s, episode steps: 11, steps per second: 287, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-1.803, 0.953], loss: 1.070382, mean_absolute_error: 2.385753, mean_q: 4.262405\n",
            " 954/1000: episode: 52, duration: 0.041s, episode steps: 10, steps per second: 242, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.111 [-2.017, 1.165], loss: 0.947532, mean_absolute_error: 2.358974, mean_q: 4.181853\n",
            " 964/1000: episode: 53, duration: 0.035s, episode steps: 10, steps per second: 289, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.111 [-2.109, 1.388], loss: 1.374099, mean_absolute_error: 2.474128, mean_q: 4.277004\n",
            " 986/1000: episode: 54, duration: 0.080s, episode steps: 22, steps per second: 274, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.056 [-1.470, 0.801], loss: 1.061733, mean_absolute_error: 2.457941, mean_q: 4.312970\n",
            " 995/1000: episode: 55, duration: 0.038s, episode steps: 9, steps per second: 234, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.122 [-1.748, 1.028], loss: 1.295621, mean_absolute_error: 2.536007, mean_q: 4.400821\n",
            "done, took 3.708 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2V7EmPbCf34v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "de2273e1-baf0-443a-98ed-8a203d17af23",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270823292,
          "user_tz": -60,
          "elapsed": 770,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 10.000, steps: 10\n",
            "Episode 3: reward: 8.000, steps: 8\n",
            "Episode 4: reward: 10.000, steps: 10\n",
            "Episode 5: reward: 8.000, steps: 8\n",
            "Episode 6: reward: 10.000, steps: 10\n",
            "Episode 7: reward: 8.000, steps: 8\n",
            "Episode 8: reward: 10.000, steps: 10\n",
            "Episode 9: reward: 8.000, steps: 8\n",
            "Episode 10: reward: 9.000, steps: 9\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 9.000, steps: 9\n",
            "Episode 13: reward: 10.000, steps: 10\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 10.000, steps: 10\n",
            "Episode 17: reward: 9.000, steps: 9\n",
            "Episode 18: reward: 9.000, steps: 9\n",
            "Episode 19: reward: 10.000, steps: 10\n",
            "Episode 20: reward: 10.000, steps: 10\n",
            "Episode 21: reward: 10.000, steps: 10\n",
            "Episode 22: reward: 10.000, steps: 10\n",
            "Episode 23: reward: 10.000, steps: 10\n",
            "Episode 24: reward: 9.000, steps: 9\n",
            "Episode 25: reward: 10.000, steps: 10\n",
            "Episode 26: reward: 10.000, steps: 10\n",
            "Episode 27: reward: 10.000, steps: 10\n",
            "Episode 28: reward: 10.000, steps: 10\n",
            "Episode 29: reward: 9.000, steps: 9\n",
            "Episode 30: reward: 9.000, steps: 9\n",
            "Episode 31: reward: 10.000, steps: 10\n",
            "Episode 32: reward: 10.000, steps: 10\n",
            "Episode 33: reward: 9.000, steps: 9\n",
            "Episode 34: reward: 9.000, steps: 9\n",
            "Episode 35: reward: 9.000, steps: 9\n",
            "Episode 36: reward: 9.000, steps: 9\n",
            "Episode 37: reward: 9.000, steps: 9\n",
            "Episode 38: reward: 9.000, steps: 9\n",
            "Episode 39: reward: 8.000, steps: 8\n",
            "Episode 40: reward: 10.000, steps: 10\n",
            "Episode 41: reward: 9.000, steps: 9\n",
            "Episode 42: reward: 10.000, steps: 10\n",
            "Episode 43: reward: 10.000, steps: 10\n",
            "Episode 44: reward: 9.000, steps: 9\n",
            "Episode 45: reward: 9.000, steps: 9\n",
            "Episode 46: reward: 9.000, steps: 9\n",
            "Episode 47: reward: 8.000, steps: 8\n",
            "Episode 48: reward: 9.000, steps: 9\n",
            "Episode 49: reward: 10.000, steps: 10\n",
            "Episode 50: reward: 8.000, steps: 8\n",
            "mean: 9.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tgFVcwMftmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Displaying test games\n",
        "\n",
        "The below cell will save just the neural net with its weights.\n",
        "\n",
        "To run the test with display:\n",
        "\n",
        "1. Go to the XWindow link, and login if you haven't already\n",
        "2. In the TightVNC terminal, type and run:\n",
        "   \n",
        "    `run <number_repeats>`\n",
        "    - replace `<number_repeats>` with however many games you want it to play (or leave it blank to run once)\n"
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model to file\n",
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}