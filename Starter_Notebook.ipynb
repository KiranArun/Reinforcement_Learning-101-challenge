{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "af5c034a-82f8-473f-ef9d-4bf65bdbebfa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979251131,
          "user_tz": -60,
          "elapsed": 15490,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# setup\n",
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Reinforcement_Learning-101-challenge': No such file or directory\n",
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 103, done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 103 (delta 57), reused 65 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 56.33 KiB | 8.05 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8cc394e4-7c7e-48af-9f3a-6858aa11543d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979264936,
          "user_tz": -60,
          "elapsed": 13744,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "!bash /content/Reinforcement_Learning-101-demo/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "get_ipython().system_raw('/content/ngrok http -config=/content/config.yml 5901 &')\n",
        "!curl -s http://localhost:4045/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XWindow Link: http://2fc8fe76.ngrok.io\n",
            "Full control one-time password: 38692308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7TgjCjUeIxz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e8435fa-3aae-47bc-d80b-e48c68e94504",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979267149,
          "user_tz": -60,
          "elapsed": 1567,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b14dfbe6-4c82-4604-f8f7-a754f4183366",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979268333,
          "user_tz": -60,
          "elapsed": 507,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "49afc7c4-873e-4b76-ad86-0d6aa371ecdd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979269160,
          "user_tz": -60,
          "elapsed": 747,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "mem = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "pol = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=mem,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=pol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkHWScERfz0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "outputId": "478e11f9-7ce5-4aa2-9bb9-23fe5f8de428",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979276114,
          "user_tz": -60,
          "elapsed": 3750,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  44/1000: episode: 1, duration: 0.064s, episode steps: 44, steps per second: 692, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.044 [-1.277, 1.158], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  65/1000: episode: 2, duration: 0.015s, episode steps: 21, steps per second: 1398, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.053 [-0.948, 1.421], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 156/1000: episode: 3, duration: 0.448s, episode steps: 91, steps per second: 203, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.008 [-0.852, 0.978], loss: 0.623247, mean_absolute_error: 0.687263, mean_q: 0.319911\n",
            " 185/1000: episode: 4, duration: 0.097s, episode steps: 29, steps per second: 298, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.120 [-1.530, 0.563], loss: 0.448996, mean_absolute_error: 0.632552, mean_q: 0.509566\n",
            " 202/1000: episode: 5, duration: 0.058s, episode steps: 17, steps per second: 292, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.079 [-0.433, 0.942], loss: 0.421807, mean_absolute_error: 0.650348, mean_q: 0.590656\n",
            " 251/1000: episode: 6, duration: 0.170s, episode steps: 49, steps per second: 288, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.048 [-0.898, 0.564], loss: 0.370738, mean_absolute_error: 0.674431, mean_q: 0.752603\n",
            " 265/1000: episode: 7, duration: 0.047s, episode steps: 14, steps per second: 299, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.090 [-0.930, 1.482], loss: 0.343012, mean_absolute_error: 0.714316, mean_q: 0.878888\n",
            " 305/1000: episode: 8, duration: 0.130s, episode steps: 40, steps per second: 307, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.505, 1.047], loss: 0.353070, mean_absolute_error: 0.767683, mean_q: 1.004587\n",
            " 321/1000: episode: 9, duration: 0.062s, episode steps: 16, steps per second: 260, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.587, 1.075], loss: 0.333041, mean_absolute_error: 0.823271, mean_q: 1.108506\n",
            " 341/1000: episode: 10, duration: 0.066s, episode steps: 20, steps per second: 304, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.097 [-0.460, 1.292], loss: 0.338001, mean_absolute_error: 0.865922, mean_q: 1.188185\n",
            " 398/1000: episode: 11, duration: 0.192s, episode steps: 57, steps per second: 297, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.033 [-0.957, 0.610], loss: 0.309826, mean_absolute_error: 0.917755, mean_q: 1.341158\n",
            " 440/1000: episode: 12, duration: 0.134s, episode steps: 42, steps per second: 314, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.196, 0.369], loss: 0.298549, mean_absolute_error: 1.002765, mean_q: 1.490916\n",
            " 533/1000: episode: 13, duration: 0.338s, episode steps: 93, steps per second: 275, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-1.122, 1.690], loss: 0.300577, mean_absolute_error: 1.126406, mean_q: 1.735758\n",
            " 559/1000: episode: 14, duration: 0.091s, episode steps: 26, steps per second: 285, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.052 [-0.619, 1.243], loss: 0.309246, mean_absolute_error: 1.243565, mean_q: 1.979827\n",
            " 585/1000: episode: 15, duration: 0.087s, episode steps: 26, steps per second: 299, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.089 [-1.166, 0.569], loss: 0.330227, mean_absolute_error: 1.311523, mean_q: 2.087426\n",
            " 603/1000: episode: 16, duration: 0.073s, episode steps: 18, steps per second: 246, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.097 [-0.799, 1.671], loss: 0.314757, mean_absolute_error: 1.342739, mean_q: 2.154184\n",
            " 633/1000: episode: 17, duration: 0.098s, episode steps: 30, steps per second: 306, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.045 [-0.859, 1.108], loss: 0.333993, mean_absolute_error: 1.404615, mean_q: 2.259634\n",
            " 681/1000: episode: 18, duration: 0.162s, episode steps: 48, steps per second: 297, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.137 [-1.790, 0.660], loss: 0.337203, mean_absolute_error: 1.477498, mean_q: 2.398953\n",
            " 708/1000: episode: 19, duration: 0.087s, episode steps: 27, steps per second: 309, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.104 [-0.584, 1.522], loss: 0.327745, mean_absolute_error: 1.535531, mean_q: 2.532873\n",
            " 728/1000: episode: 20, duration: 0.070s, episode steps: 20, steps per second: 284, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.075 [-0.786, 1.568], loss: 0.343417, mean_absolute_error: 1.588629, mean_q: 2.611128\n",
            " 748/1000: episode: 21, duration: 0.076s, episode steps: 20, steps per second: 262, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.059 [-1.135, 1.805], loss: 0.339428, mean_absolute_error: 1.625688, mean_q: 2.698400\n",
            " 765/1000: episode: 22, duration: 0.056s, episode steps: 17, steps per second: 301, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.120 [-0.550, 1.070], loss: 0.355693, mean_absolute_error: 1.663861, mean_q: 2.784239\n",
            " 782/1000: episode: 23, duration: 0.056s, episode steps: 17, steps per second: 306, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.100 [-1.521, 0.768], loss: 0.382979, mean_absolute_error: 1.703237, mean_q: 2.838523\n",
            " 795/1000: episode: 24, duration: 0.045s, episode steps: 13, steps per second: 291, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.099 [-1.747, 2.767], loss: 0.397314, mean_absolute_error: 1.748745, mean_q: 2.911903\n",
            " 808/1000: episode: 25, duration: 0.054s, episode steps: 13, steps per second: 239, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.100 [-1.393, 2.206], loss: 0.432863, mean_absolute_error: 1.806390, mean_q: 2.979579\n",
            " 819/1000: episode: 26, duration: 0.042s, episode steps: 11, steps per second: 260, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.140 [-1.307, 0.740], loss: 0.389921, mean_absolute_error: 1.816438, mean_q: 3.019549\n",
            " 865/1000: episode: 27, duration: 0.148s, episode steps: 46, steps per second: 310, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.044 [-1.137, 1.368], loss: 0.422788, mean_absolute_error: 1.879150, mean_q: 3.130200\n",
            " 895/1000: episode: 28, duration: 0.112s, episode steps: 30, steps per second: 268, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.061 [-0.998, 1.913], loss: 0.455176, mean_absolute_error: 1.951877, mean_q: 3.246252\n",
            " 934/1000: episode: 29, duration: 0.126s, episode steps: 39, steps per second: 309, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.056 [-0.624, 1.491], loss: 0.470124, mean_absolute_error: 2.026664, mean_q: 3.384855\n",
            "done, took 3.438 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2V7EmPbCf34v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "c15dab43-d5ac-4608-96f9-14b68da0f7d3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979276996,
          "user_tz": -60,
          "elapsed": 774,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 13.000, steps: 13\n",
            "Episode 2: reward: 11.000, steps: 11\n",
            "Episode 3: reward: 15.000, steps: 15\n",
            "Episode 4: reward: 11.000, steps: 11\n",
            "Episode 5: reward: 16.000, steps: 16\n",
            "Episode 6: reward: 13.000, steps: 13\n",
            "Episode 7: reward: 14.000, steps: 14\n",
            "Episode 8: reward: 18.000, steps: 18\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 14.000, steps: 14\n",
            "Episode 11: reward: 12.000, steps: 12\n",
            "Episode 12: reward: 13.000, steps: 13\n",
            "Episode 13: reward: 15.000, steps: 15\n",
            "Episode 14: reward: 14.000, steps: 14\n",
            "Episode 15: reward: 17.000, steps: 17\n",
            "Episode 16: reward: 13.000, steps: 13\n",
            "Episode 17: reward: 11.000, steps: 11\n",
            "Episode 18: reward: 16.000, steps: 16\n",
            "Episode 19: reward: 14.000, steps: 14\n",
            "Episode 20: reward: 14.000, steps: 14\n",
            "Episode 21: reward: 13.000, steps: 13\n",
            "Episode 22: reward: 14.000, steps: 14\n",
            "Episode 23: reward: 16.000, steps: 16\n",
            "Episode 24: reward: 15.000, steps: 15\n",
            "Episode 25: reward: 16.000, steps: 16\n",
            "Episode 26: reward: 14.000, steps: 14\n",
            "Episode 27: reward: 13.000, steps: 13\n",
            "Episode 28: reward: 13.000, steps: 13\n",
            "Episode 29: reward: 15.000, steps: 15\n",
            "Episode 30: reward: 13.000, steps: 13\n",
            "Episode 31: reward: 15.000, steps: 15\n",
            "Episode 32: reward: 13.000, steps: 13\n",
            "Episode 33: reward: 14.000, steps: 14\n",
            "Episode 34: reward: 15.000, steps: 15\n",
            "Episode 35: reward: 12.000, steps: 12\n",
            "Episode 36: reward: 11.000, steps: 11\n",
            "Episode 37: reward: 17.000, steps: 17\n",
            "Episode 38: reward: 12.000, steps: 12\n",
            "Episode 39: reward: 13.000, steps: 13\n",
            "Episode 40: reward: 12.000, steps: 12\n",
            "Episode 41: reward: 17.000, steps: 17\n",
            "Episode 42: reward: 12.000, steps: 12\n",
            "Episode 43: reward: 17.000, steps: 17\n",
            "Episode 44: reward: 15.000, steps: 15\n",
            "Episode 45: reward: 12.000, steps: 12\n",
            "Episode 46: reward: 15.000, steps: 15\n",
            "Episode 47: reward: 12.000, steps: 12\n",
            "Episode 48: reward: 16.000, steps: 16\n",
            "Episode 49: reward: 15.000, steps: 15\n",
            "Episode 50: reward: 18.000, steps: 18\n",
            "mean: 13.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tgFVcwMftmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Displaying test games\n",
        "\n",
        "The below cell will save just the neural net with its weights.\n",
        "\n",
        "To run the test with display:\n",
        "\n",
        "1. Go to the XWindow link, and login if you haven't already\n",
        "2. In the TightVNC terminal, type and run:\n",
        "   \n",
        "    `run <number_repeats>`\n",
        "    - replace `<number_repeats>` with however many games you want it to play (or leave it blank to run once)\n"
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model to file\n",
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}