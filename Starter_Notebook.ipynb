{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT © 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "zZpfzlLPFN1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "YmJSJ3QkRFyw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "1ba78f27-745e-4794-e222-7309e6a760ff",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590779627,
          "user_tz": -60,
          "elapsed": 12999,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# setup\n",
        "!rm -r Reinforcement_Learning-101-challenge\n",
        "!git clone https://github.com/KiranArun/Reinforcement_Learning-101-challenge.git 1>/dev/null\n",
        "!bash Reinforcement_Learning-101-challenge/scripts/setup.sh"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement_Learning-101-challenge'...\n",
            "remote: Counting objects: 87, done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 87 (delta 47), reused 55 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "mkdir: cannot create directory ‘/content/models/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DjFgb_wrXMLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97772cfc-d3d4-436f-878d-3ca9a059819c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590845415,
          "user_tz": -60,
          "elapsed": 13671,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DEFINITELY DO NOT EDIT THIS CELL\n",
        "\n",
        "import os\n",
        "# Start noVNC remote Xwindows in a browser with TurboVNC\n",
        "!bash /content/Reinforcement_Learning-101-challenge/scripts/kill-novnc.sh >/dev/null\n",
        "!sleep 2\n",
        "# start novnc server and expose via ngrok NOT HTTPS\n",
        "os.environ['PATH'] += \"${PATH}:/opt/VirtualGL/bin:/opt/TurboVNC/bin\"\n",
        "# hack line below to get vnc to start as ipython subprocess\n",
        "!timeout 5 /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup 2>/dev/null\n",
        "get_ipython().system_raw('python /opt/websockify/run 5901 --web=/opt/noVNC --wrap-mode=ignore -- /opt/TurboVNC/bin/vncserver :1 -securitytypes otp -otp -noxstartup > /content/.vnc/stdout 2>&1 &')\n",
        "# ngrok remote tunnel\n",
        "get_ipython().system_raw('/content/ngrok http 5901 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print('XWindow Link:', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "# get Xwindows going (right click to get openbox menu)\n",
        "get_ipython().system_raw('DISPLAY=:1 openbox &')\n",
        "# one-time-password\n",
        "!grep \"one-time password:\" /content/.vnc/stdout"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XWindow Link: http://a81125d2.ngrok.io\n",
            "Full control one-time password: 23164052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw2Nsh0GKkJb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT THIS CELL\n",
        "\n",
        "# set up Xvfb and attach it's own vncserver and test with spinning gears\n",
        "!pkill Xvfb   # clean-up\n",
        "!pkill x11vnc # clean-up\n",
        "\n",
        "get_ipython().system_raw('DISPLAY=:1 xterm &')\n",
        "get_ipython().system_raw('/usr/bin/Xvfb :99 -screen 0 640x480x24 &')\n",
        "get_ipython().system_raw('/usr/bin/x11vnc -rfbport 5902 -forever -display :99 &')\n",
        "!sleep 2\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 glxgears &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMV21EgFULIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# open a terminal into the glxgears session using TightVNC\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -e /bin/bash -l -c \"pkill glxgears\"')\n",
        "get_ipython().system_raw('DISPLAY=:1 /usr/bin/vncviewer localhost:5902 &')\n",
        "get_ipython().system_raw('DISPLAY=:99 xterm -geometry 105x35 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52a6d151-675f-4792-ead2-f1c42aa1bbdf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590874123,
          "user_tz": -60,
          "elapsed": 461,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "5d3106c5-f81a-4b64-a145-31879a92f660",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590874767,
          "user_tz": -60,
          "elapsed": 510,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkHWScERfz0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "90170ad7-2db7-4ea3-f1f2-816393c4b69c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590882054,
          "user_tz": -60,
          "elapsed": 4074,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  10/1000: episode: 1, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.133 [-1.657, 0.968], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  61/1000: episode: 2, duration: 0.032s, episode steps: 51, steps per second: 1572, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.097 [-1.316, 1.350], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  86/1000: episode: 3, duration: 0.016s, episode steps: 25, steps per second: 1608, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.014 [-1.782, 1.393], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 101/1000: episode: 4, duration: 0.282s, episode steps: 15, steps per second: 53, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-0.795, 1.482], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 124/1000: episode: 5, duration: 0.090s, episode steps: 23, steps per second: 255, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.090 [-0.806, 1.156], loss: 0.588799, mean_absolute_error: 0.633768, mean_q: 0.222357\n",
            " 142/1000: episode: 6, duration: 0.067s, episode steps: 18, steps per second: 268, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.448, 0.935], loss: 0.499026, mean_absolute_error: 0.607569, mean_q: 0.352900\n",
            " 199/1000: episode: 7, duration: 0.212s, episode steps: 57, steps per second: 269, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.050 [-1.320, 1.426], loss: 0.397344, mean_absolute_error: 0.603775, mean_q: 0.507287\n",
            " 219/1000: episode: 8, duration: 0.072s, episode steps: 20, steps per second: 279, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.087 [-0.634, 1.380], loss: 0.345393, mean_absolute_error: 0.655639, mean_q: 0.695725\n",
            " 242/1000: episode: 9, duration: 0.090s, episode steps: 23, steps per second: 257, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.062 [-1.145, 1.937], loss: 0.325053, mean_absolute_error: 0.686197, mean_q: 0.784056\n",
            " 257/1000: episode: 10, duration: 0.063s, episode steps: 15, steps per second: 236, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-1.181, 2.029], loss: 0.312175, mean_absolute_error: 0.712476, mean_q: 0.862540\n",
            " 275/1000: episode: 11, duration: 0.066s, episode steps: 18, steps per second: 273, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.060 [-1.553, 2.464], loss: 0.325527, mean_absolute_error: 0.760901, mean_q: 0.925795\n",
            " 289/1000: episode: 12, duration: 0.051s, episode steps: 14, steps per second: 274, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.064 [-2.376, 1.588], loss: 0.325765, mean_absolute_error: 0.806283, mean_q: 1.021643\n",
            " 352/1000: episode: 13, duration: 0.239s, episode steps: 63, steps per second: 264, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.017 [-1.850, 1.013], loss: 0.304492, mean_absolute_error: 0.861503, mean_q: 1.132650\n",
            " 391/1000: episode: 14, duration: 0.129s, episode steps: 39, steps per second: 301, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.096 [-0.584, 0.951], loss: 0.302890, mean_absolute_error: 0.954617, mean_q: 1.310784\n",
            " 441/1000: episode: 15, duration: 0.159s, episode steps: 50, steps per second: 314, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: -0.126 [-2.883, 2.884], loss: 0.304380, mean_absolute_error: 1.034456, mean_q: 1.474650\n",
            " 463/1000: episode: 16, duration: 0.073s, episode steps: 22, steps per second: 303, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.102 [-0.562, 1.123], loss: 0.292810, mean_absolute_error: 1.081064, mean_q: 1.606623\n",
            " 504/1000: episode: 17, duration: 0.139s, episode steps: 41, steps per second: 295, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: -0.065 [-1.755, 2.172], loss: 0.316091, mean_absolute_error: 1.169659, mean_q: 1.771857\n",
            " 525/1000: episode: 18, duration: 0.068s, episode steps: 21, steps per second: 308, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.062 [-1.146, 1.801], loss: 0.306928, mean_absolute_error: 1.214187, mean_q: 1.890800\n",
            " 569/1000: episode: 19, duration: 0.138s, episode steps: 44, steps per second: 320, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-0.940, 0.502], loss: 0.320807, mean_absolute_error: 1.293157, mean_q: 2.061875\n",
            " 592/1000: episode: 20, duration: 0.084s, episode steps: 23, steps per second: 272, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.052 [-1.380, 0.840], loss: 0.320497, mean_absolute_error: 1.355111, mean_q: 2.168379\n",
            " 602/1000: episode: 21, duration: 0.036s, episode steps: 10, steps per second: 279, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.204, 2.023], loss: 0.313850, mean_absolute_error: 1.365470, mean_q: 2.211696\n",
            " 616/1000: episode: 22, duration: 0.049s, episode steps: 14, steps per second: 284, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.098 [-1.542, 2.511], loss: 0.351862, mean_absolute_error: 1.433048, mean_q: 2.254625\n",
            " 656/1000: episode: 23, duration: 0.145s, episode steps: 40, steps per second: 277, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-1.583, 1.495], loss: 0.324804, mean_absolute_error: 1.443961, mean_q: 2.355548\n",
            " 693/1000: episode: 24, duration: 0.137s, episode steps: 37, steps per second: 270, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.017 [-1.023, 1.604], loss: 0.344750, mean_absolute_error: 1.501521, mean_q: 2.473132\n",
            " 708/1000: episode: 25, duration: 0.057s, episode steps: 15, steps per second: 263, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.118 [-1.153, 1.995], loss: 0.361414, mean_absolute_error: 1.571529, mean_q: 2.585896\n",
            " 739/1000: episode: 26, duration: 0.117s, episode steps: 31, steps per second: 264, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.323 [0.000, 1.000], mean observation: -0.012 [-2.294, 3.017], loss: 0.388969, mean_absolute_error: 1.634103, mean_q: 2.711405\n",
            " 779/1000: episode: 27, duration: 0.146s, episode steps: 40, steps per second: 273, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.094 [-0.577, 0.919], loss: 0.375685, mean_absolute_error: 1.704348, mean_q: 2.853460\n",
            " 801/1000: episode: 28, duration: 0.092s, episode steps: 22, steps per second: 240, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.057 [-1.391, 2.150], loss: 0.414848, mean_absolute_error: 1.793898, mean_q: 3.007936\n",
            " 839/1000: episode: 29, duration: 0.134s, episode steps: 38, steps per second: 283, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: -0.009 [-1.536, 2.115], loss: 0.406263, mean_absolute_error: 1.830316, mean_q: 3.067489\n",
            " 860/1000: episode: 30, duration: 0.083s, episode steps: 21, steps per second: 252, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.046 [-1.564, 2.356], loss: 0.396723, mean_absolute_error: 1.877837, mean_q: 3.187442\n",
            " 873/1000: episode: 31, duration: 0.054s, episode steps: 13, steps per second: 240, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.805, 1.543], loss: 0.383574, mean_absolute_error: 1.897833, mean_q: 3.250605\n",
            " 909/1000: episode: 32, duration: 0.131s, episode steps: 36, steps per second: 274, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.044 [-0.619, 1.273], loss: 0.413790, mean_absolute_error: 1.960439, mean_q: 3.331743\n",
            " 925/1000: episode: 33, duration: 0.067s, episode steps: 16, steps per second: 239, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.570, 2.516], loss: 0.446902, mean_absolute_error: 2.017414, mean_q: 3.425448\n",
            " 941/1000: episode: 34, duration: 0.058s, episode steps: 16, steps per second: 276, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.069 [-2.061, 1.358], loss: 0.453567, mean_absolute_error: 2.048097, mean_q: 3.511838\n",
            " 958/1000: episode: 35, duration: 0.064s, episode steps: 17, steps per second: 267, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.084 [-0.949, 1.560], loss: 0.557430, mean_absolute_error: 2.091930, mean_q: 3.524620\n",
            " 970/1000: episode: 36, duration: 0.051s, episode steps: 12, steps per second: 236, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.540, 2.562], loss: 0.447847, mean_absolute_error: 2.087533, mean_q: 3.572038\n",
            " 984/1000: episode: 37, duration: 0.062s, episode steps: 14, steps per second: 226, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.079 [-0.645, 1.228], loss: 0.455474, mean_absolute_error: 2.124305, mean_q: 3.626592\n",
            "done, took 3.693 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2V7EmPbCf34v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "49cea00c-f915-4167-e798-0b7a63bcc0bf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527590885320,
          "user_tz": -60,
          "elapsed": 738,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 8.000, steps: 8\n",
            "Episode 3: reward: 9.000, steps: 9\n",
            "Episode 4: reward: 10.000, steps: 10\n",
            "Episode 5: reward: 8.000, steps: 8\n",
            "Episode 6: reward: 10.000, steps: 10\n",
            "Episode 7: reward: 10.000, steps: 10\n",
            "Episode 8: reward: 10.000, steps: 10\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 10.000, steps: 10\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 10.000, steps: 10\n",
            "Episode 13: reward: 9.000, steps: 9\n",
            "Episode 14: reward: 9.000, steps: 9\n",
            "Episode 15: reward: 8.000, steps: 8\n",
            "Episode 16: reward: 8.000, steps: 8\n",
            "Episode 17: reward: 10.000, steps: 10\n",
            "Episode 18: reward: 9.000, steps: 9\n",
            "Episode 19: reward: 10.000, steps: 10\n",
            "Episode 20: reward: 11.000, steps: 11\n",
            "Episode 21: reward: 9.000, steps: 9\n",
            "Episode 22: reward: 8.000, steps: 8\n",
            "Episode 23: reward: 10.000, steps: 10\n",
            "Episode 24: reward: 11.000, steps: 11\n",
            "Episode 25: reward: 9.000, steps: 9\n",
            "Episode 26: reward: 11.000, steps: 11\n",
            "Episode 27: reward: 9.000, steps: 9\n",
            "Episode 28: reward: 9.000, steps: 9\n",
            "Episode 29: reward: 11.000, steps: 11\n",
            "Episode 30: reward: 10.000, steps: 10\n",
            "Episode 31: reward: 9.000, steps: 9\n",
            "Episode 32: reward: 9.000, steps: 9\n",
            "Episode 33: reward: 9.000, steps: 9\n",
            "Episode 34: reward: 10.000, steps: 10\n",
            "Episode 35: reward: 10.000, steps: 10\n",
            "Episode 36: reward: 10.000, steps: 10\n",
            "Episode 37: reward: 9.000, steps: 9\n",
            "Episode 38: reward: 9.000, steps: 9\n",
            "Episode 39: reward: 9.000, steps: 9\n",
            "Episode 40: reward: 9.000, steps: 9\n",
            "Episode 41: reward: 10.000, steps: 10\n",
            "Episode 42: reward: 9.000, steps: 9\n",
            "Episode 43: reward: 8.000, steps: 8\n",
            "Episode 44: reward: 9.000, steps: 9\n",
            "Episode 45: reward: 9.000, steps: 9\n",
            "Episode 46: reward: 8.000, steps: 8\n",
            "Episode 47: reward: 10.000, steps: 10\n",
            "Episode 48: reward: 10.000, steps: 10\n",
            "Episode 49: reward: 10.000, steps: 10\n",
            "Episode 50: reward: 10.000, steps: 10\n",
            "mean: 9.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tgFVcwMftmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Displaying test games\n",
        "\n",
        "The below cell will save just the neural net with its weights.\n",
        "\n",
        "To run the test with display:\n",
        "\n",
        "1. Go to the XWindow link, and login if you haven't already\n",
        "2. In the TightVNC terminal, run:\n",
        "\n",
        "    - `python3 Reinforcement_Learning-101-challenge/display_cartpole.py <number_repeats>`\n",
        "    - replace `<number_repeats>` with however many games you want it to play\n"
      ]
    },
    {
      "metadata": {
        "id": "bE1EVZLHWbSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model to file\n",
        "nn.save('/content/models/nn.hdf5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}