# -*- coding: utf-8 -*-
"""Starter_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16kFf9mcm1muJTyyi_R5TWZUUKE5J_5u2

COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>

### Setup
"""

import os

"""# RL Challenge"""

# importing libraries
import numpy as np
import matplotlib.pyplot as plt
import gym

import keras
import keras.layers as layers
from rl import agents,memory,policy

"""### Hyperparameters"""

# Hyperparams

# number of steps to keep in experience buffer
memory_limit = 400

# discount value
gamma = 0.99

# how much to update target graph
target_model_update = 1e-2

# learning rate
learning_rate = 1e-2

# number of steps to sample from buffer to train on
batch_size = 32

# DO NOT EDIT

# setting environment
env = gym.make('CartPole-v1')
num_actions = env.action_space.n

"""### Neural Network"""

keras.backend.clear_session()

# create neural net
model = keras.models.Sequential()

# keep this as input layer
model.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))

# keep this as output layer
model.add(layers.Dense(num_actions))

model.summary()

"""### Optimizer

https://keras.io/optimizers/
"""

# set optimizer
optimizer = keras.optimizers.SGD(lr=learning_rate)

"""### Memory"""

# DO NOT EDIT

# set experience buffer
memory = memory.SequentialMemory(limit=memory_limit,
                                    window_length=1)

"""### Policy

choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py

[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)
"""

# set policy
policy = policy.BoltzmannQPolicy()

"""### Agent

choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents

and use: https://github.com/keras-rl/keras-rl/tree/master/examples
"""

# create agent
model = agents.dqn.DQNAgent(model=model,
                            nb_actions=num_actions,
                            memory=memory,
                            gamma=gamma,
                            batch_size=batch_size,
                            nb_steps_warmup=100,
                            target_model_update=target_model_update,
                            policy=policy)

# DO NOT EDIT

# compile model
model.compile(optimizer,
            metrics=['mae'])

# DO NOT EDIT

# train
history = model.fit(env,
                  nb_steps=1000,
                  verbose=2)

# DO NOT EDIT

# test
hist = model.test(env, nb_episodes=500, visualize=True)
print('mean:', np.mean(hist.history['episode_reward']))

# start rendering
env.render(mode='human')
