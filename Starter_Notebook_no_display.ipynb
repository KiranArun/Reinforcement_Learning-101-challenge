{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook_no_display.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "wApqhsthcQZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0f5qGxCG1L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a564f173-5639-4288-e8f5-05c819e780ab",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979640879,
          "user_tz": -60,
          "elapsed": 5355,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install gym\n",
        "!pip3 install keras-rl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.1.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f5df093-a2d7-4de8-8b2a-6ef651f93bca",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979642568,
          "user_tz": -60,
          "elapsed": 1600,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "894186e1-8f09-4f97-ada3-873d0a1ed11f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979643876,
          "user_tz": -60,
          "elapsed": 417,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "75fbdee9-bd21-4c08-d4e0-447ff827b1ec",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979644444,
          "user_tz": -60,
          "elapsed": 450,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "mem = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "pol = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=mem,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=pol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiLm78fBf7Gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "outputId": "8f083364-9235-4b79-e4b6-594c16860060",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979651694,
          "user_tz": -60,
          "elapsed": 4006,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  56/1000: episode: 1, duration: 0.075s, episode steps: 56, steps per second: 751, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: -0.003 [-2.012, 1.342], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 104/1000: episode: 2, duration: 0.283s, episode steps: 48, steps per second: 170, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.604 [0.000, 1.000], mean observation: 0.145 [-2.212, 2.289], loss: 0.511626, mean_absolute_error: 0.709396, mean_q: -0.128323\n",
            " 116/1000: episode: 3, duration: 0.042s, episode steps: 12, steps per second: 286, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.855, 1.148], loss: 0.527065, mean_absolute_error: 0.690373, mean_q: -0.051019\n",
            " 162/1000: episode: 4, duration: 0.154s, episode steps: 46, steps per second: 299, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.413 [0.000, 1.000], mean observation: -0.098 [-1.764, 1.880], loss: 0.409939, mean_absolute_error: 0.630720, mean_q: 0.035434\n",
            " 178/1000: episode: 5, duration: 0.062s, episode steps: 16, steps per second: 258, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.079 [-1.213, 0.811], loss: 0.335825, mean_absolute_error: 0.622023, mean_q: 0.300887\n",
            " 240/1000: episode: 6, duration: 0.189s, episode steps: 62, steps per second: 327, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.089 [-0.643, 1.172], loss: 0.299647, mean_absolute_error: 0.621475, mean_q: 0.445601\n",
            " 264/1000: episode: 7, duration: 0.088s, episode steps: 24, steps per second: 273, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.097 [-0.794, 1.299], loss: 0.269046, mean_absolute_error: 0.672013, mean_q: 0.666095\n",
            " 272/1000: episode: 8, duration: 0.028s, episode steps: 8, steps per second: 281, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.564, 1.562], loss: 0.268127, mean_absolute_error: 0.685037, mean_q: 0.706458\n",
            " 294/1000: episode: 9, duration: 0.073s, episode steps: 22, steps per second: 300, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.080 [-1.140, 2.073], loss: 0.275475, mean_absolute_error: 0.724086, mean_q: 0.780643\n",
            " 311/1000: episode: 10, duration: 0.055s, episode steps: 17, steps per second: 309, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.075 [-0.821, 1.541], loss: 0.270808, mean_absolute_error: 0.767757, mean_q: 0.897309\n",
            " 330/1000: episode: 11, duration: 0.073s, episode steps: 19, steps per second: 259, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.062 [-1.195, 1.962], loss: 0.270686, mean_absolute_error: 0.807681, mean_q: 0.983122\n",
            " 346/1000: episode: 12, duration: 0.051s, episode steps: 16, steps per second: 313, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.077 [-1.857, 1.141], loss: 0.296009, mean_absolute_error: 0.868483, mean_q: 1.079163\n",
            " 364/1000: episode: 13, duration: 0.057s, episode steps: 18, steps per second: 317, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.097 [-2.231, 1.349], loss: 0.271508, mean_absolute_error: 0.872935, mean_q: 1.113146\n",
            " 378/1000: episode: 14, duration: 0.048s, episode steps: 14, steps per second: 295, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-0.975, 1.624], loss: 0.276991, mean_absolute_error: 0.891858, mean_q: 1.157000\n",
            " 408/1000: episode: 15, duration: 0.104s, episode steps: 30, steps per second: 288, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.046 [-2.154, 1.503], loss: 0.275420, mean_absolute_error: 0.928814, mean_q: 1.237925\n",
            " 457/1000: episode: 16, duration: 0.160s, episode steps: 49, steps per second: 307, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.388 [0.000, 1.000], mean observation: 0.006 [-2.122, 2.970], loss: 0.281109, mean_absolute_error: 1.013761, mean_q: 1.410052\n",
            " 480/1000: episode: 17, duration: 0.093s, episode steps: 23, steps per second: 247, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.100 [-0.563, 0.932], loss: 0.292816, mean_absolute_error: 1.079458, mean_q: 1.548693\n",
            " 498/1000: episode: 18, duration: 0.057s, episode steps: 18, steps per second: 316, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-0.787, 1.214], loss: 0.304049, mean_absolute_error: 1.149909, mean_q: 1.670202\n",
            " 514/1000: episode: 19, duration: 0.050s, episode steps: 16, steps per second: 318, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.078 [-0.962, 1.492], loss: 0.306052, mean_absolute_error: 1.192120, mean_q: 1.746275\n",
            " 536/1000: episode: 20, duration: 0.071s, episode steps: 22, steps per second: 308, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.024 [-1.734, 2.467], loss: 0.298150, mean_absolute_error: 1.221212, mean_q: 1.830500\n",
            " 548/1000: episode: 21, duration: 0.045s, episode steps: 12, steps per second: 268, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.110 [-1.773, 2.693], loss: 0.308419, mean_absolute_error: 1.257915, mean_q: 1.894410\n",
            " 562/1000: episode: 22, duration: 0.053s, episode steps: 14, steps per second: 264, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.076 [-0.814, 1.416], loss: 0.304977, mean_absolute_error: 1.300787, mean_q: 1.987594\n",
            " 582/1000: episode: 23, duration: 0.063s, episode steps: 20, steps per second: 318, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.057 [-1.228, 2.030], loss: 0.320724, mean_absolute_error: 1.343735, mean_q: 2.083856\n",
            " 602/1000: episode: 24, duration: 0.065s, episode steps: 20, steps per second: 307, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.068 [-1.962, 3.086], loss: 0.333714, mean_absolute_error: 1.389088, mean_q: 2.163484\n",
            " 612/1000: episode: 25, duration: 0.040s, episode steps: 10, steps per second: 251, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.115 [-1.552, 2.339], loss: 0.362422, mean_absolute_error: 1.443401, mean_q: 2.259937\n",
            " 622/1000: episode: 26, duration: 0.041s, episode steps: 10, steps per second: 245, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.575, 2.626], loss: 0.336711, mean_absolute_error: 1.459724, mean_q: 2.329061\n",
            " 637/1000: episode: 27, duration: 0.050s, episode steps: 15, steps per second: 303, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.092 [-1.333, 0.656], loss: 0.373017, mean_absolute_error: 1.477352, mean_q: 2.381763\n",
            " 654/1000: episode: 28, duration: 0.055s, episode steps: 17, steps per second: 309, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.070 [-1.744, 2.695], loss: 0.379752, mean_absolute_error: 1.513427, mean_q: 2.441083\n",
            " 670/1000: episode: 29, duration: 0.135s, episode steps: 16, steps per second: 118, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.063 [-1.449, 0.957], loss: 0.429407, mean_absolute_error: 1.564001, mean_q: 2.538133\n",
            " 683/1000: episode: 30, duration: 0.044s, episode steps: 13, steps per second: 293, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.116 [-1.328, 2.037], loss: 0.423325, mean_absolute_error: 1.591044, mean_q: 2.594419\n",
            " 713/1000: episode: 31, duration: 0.104s, episode steps: 30, steps per second: 290, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-0.612, 1.285], loss: 0.383535, mean_absolute_error: 1.597215, mean_q: 2.624875\n",
            " 725/1000: episode: 32, duration: 0.041s, episode steps: 12, steps per second: 293, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.332, 2.202], loss: 0.425718, mean_absolute_error: 1.647624, mean_q: 2.709227\n",
            " 742/1000: episode: 33, duration: 0.071s, episode steps: 17, steps per second: 239, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.074 [-1.197, 1.832], loss: 0.447992, mean_absolute_error: 1.712674, mean_q: 2.809613\n",
            " 754/1000: episode: 34, duration: 0.041s, episode steps: 12, steps per second: 294, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-1.190, 1.913], loss: 0.448060, mean_absolute_error: 1.751459, mean_q: 2.942328\n",
            " 772/1000: episode: 35, duration: 0.059s, episode steps: 18, steps per second: 305, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.055 [-1.988, 3.057], loss: 0.456744, mean_absolute_error: 1.776218, mean_q: 2.997100\n",
            " 789/1000: episode: 36, duration: 0.057s, episode steps: 17, steps per second: 298, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.063 [-1.383, 2.143], loss: 0.521635, mean_absolute_error: 1.848543, mean_q: 3.081003\n",
            " 802/1000: episode: 37, duration: 0.049s, episode steps: 13, steps per second: 265, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.115 [-1.824, 1.008], loss: 0.531506, mean_absolute_error: 1.907038, mean_q: 3.265583\n",
            " 814/1000: episode: 38, duration: 0.044s, episode steps: 12, steps per second: 271, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.104 [-0.990, 1.466], loss: 0.557468, mean_absolute_error: 1.920529, mean_q: 3.270222\n",
            " 843/1000: episode: 39, duration: 0.094s, episode steps: 29, steps per second: 310, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.078 [-0.491, 1.004], loss: 0.569994, mean_absolute_error: 1.953883, mean_q: 3.363709\n",
            " 854/1000: episode: 40, duration: 0.036s, episode steps: 11, steps per second: 306, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.123 [-0.764, 1.446], loss: 0.582317, mean_absolute_error: 1.991120, mean_q: 3.385006\n",
            " 866/1000: episode: 41, duration: 0.050s, episode steps: 12, steps per second: 241, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.108 [-1.567, 2.621], loss: 0.517943, mean_absolute_error: 1.976851, mean_q: 3.397832\n",
            " 883/1000: episode: 42, duration: 0.059s, episode steps: 17, steps per second: 287, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.078 [-1.769, 2.772], loss: 0.642119, mean_absolute_error: 2.057055, mean_q: 3.510746\n",
            " 899/1000: episode: 43, duration: 0.054s, episode steps: 16, steps per second: 298, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.077 [-1.419, 2.179], loss: 0.683842, mean_absolute_error: 2.114185, mean_q: 3.630483\n",
            " 915/1000: episode: 44, duration: 0.054s, episode steps: 16, steps per second: 294, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.079 [-0.810, 1.549], loss: 0.715092, mean_absolute_error: 2.118203, mean_q: 3.635434\n",
            " 925/1000: episode: 45, duration: 0.043s, episode steps: 10, steps per second: 231, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.190, 2.095], loss: 0.667268, mean_absolute_error: 2.134512, mean_q: 3.714100\n",
            " 947/1000: episode: 46, duration: 0.074s, episode steps: 22, steps per second: 297, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.061 [-1.353, 0.797], loss: 0.731942, mean_absolute_error: 2.163801, mean_q: 3.737441\n",
            " 962/1000: episode: 47, duration: 0.050s, episode steps: 15, steps per second: 299, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.090 [-1.596, 2.541], loss: 0.738843, mean_absolute_error: 2.170900, mean_q: 3.717442\n",
            " 972/1000: episode: 48, duration: 0.036s, episode steps: 10, steps per second: 282, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.551, 2.622], loss: 0.785407, mean_absolute_error: 2.250853, mean_q: 3.898109\n",
            " 981/1000: episode: 49, duration: 0.032s, episode steps: 9, steps per second: 285, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.128 [-1.415, 2.236], loss: 0.807587, mean_absolute_error: 2.235417, mean_q: 3.823614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "done, took 3.560 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5-xJqFVPf87m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "dc037a0d-3999-4f03-bee8-8800646faa31",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530979652601,
          "user_tz": -60,
          "elapsed": 873,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 11.000, steps: 11\n",
            "Episode 2: reward: 9.000, steps: 9\n",
            "Episode 3: reward: 10.000, steps: 10\n",
            "Episode 4: reward: 9.000, steps: 9\n",
            "Episode 5: reward: 10.000, steps: 10\n",
            "Episode 6: reward: 10.000, steps: 10\n",
            "Episode 7: reward: 10.000, steps: 10\n",
            "Episode 8: reward: 9.000, steps: 9\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 11.000, steps: 11\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 10.000, steps: 10\n",
            "Episode 13: reward: 9.000, steps: 9\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 8.000, steps: 8\n",
            "Episode 17: reward: 9.000, steps: 9\n",
            "Episode 18: reward: 10.000, steps: 10\n",
            "Episode 19: reward: 9.000, steps: 9\n",
            "Episode 20: reward: 10.000, steps: 10\n",
            "Episode 21: reward: 10.000, steps: 10\n",
            "Episode 22: reward: 9.000, steps: 9\n",
            "Episode 23: reward: 10.000, steps: 10\n",
            "Episode 24: reward: 9.000, steps: 9\n",
            "Episode 25: reward: 10.000, steps: 10\n",
            "Episode 26: reward: 9.000, steps: 9\n",
            "Episode 27: reward: 9.000, steps: 9\n",
            "Episode 28: reward: 8.000, steps: 8\n",
            "Episode 29: reward: 9.000, steps: 9\n",
            "Episode 30: reward: 10.000, steps: 10\n",
            "Episode 31: reward: 9.000, steps: 9\n",
            "Episode 32: reward: 10.000, steps: 10\n",
            "Episode 33: reward: 11.000, steps: 11\n",
            "Episode 34: reward: 8.000, steps: 8\n",
            "Episode 35: reward: 10.000, steps: 10\n",
            "Episode 36: reward: 9.000, steps: 9\n",
            "Episode 37: reward: 10.000, steps: 10\n",
            "Episode 38: reward: 9.000, steps: 9\n",
            "Episode 39: reward: 10.000, steps: 10\n",
            "Episode 40: reward: 10.000, steps: 10\n",
            "Episode 41: reward: 8.000, steps: 8\n",
            "Episode 42: reward: 10.000, steps: 10\n",
            "Episode 43: reward: 10.000, steps: 10\n",
            "Episode 44: reward: 11.000, steps: 11\n",
            "Episode 45: reward: 10.000, steps: 10\n",
            "Episode 46: reward: 10.000, steps: 10\n",
            "Episode 47: reward: 10.000, steps: 10\n",
            "Episode 48: reward: 9.000, steps: 9\n",
            "Episode 49: reward: 10.000, steps: 10\n",
            "Episode 50: reward: 10.000, steps: 10\n",
            "mean: 9.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PoP-VNm4CjeC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}