{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook_no_display.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "wApqhsthcQZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0f5qGxCG1L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "b248a4d2-519b-48e9-c52a-3b1903a97727",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270321169,
          "user_tz": -60,
          "elapsed": 7021,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install gym\n",
        "!pip3 install keras-rl"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\r\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.3)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\r\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.12)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (0.19.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3decf70d-4fc1-46e6-b495-ac67d7d78088",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270323195,
          "user_tz": -60,
          "elapsed": 522,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "aa61b045-e1a3-48fc-9cc9-dc66bb9564b3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270323860,
          "user_tz": -60,
          "elapsed": 618,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "nn = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "nn.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "nn.add(layers.Dense(num_actions))\n",
        "\n",
        "nn.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "mem = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "pol = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=nn,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=mem,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=pol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiLm78fBf7Gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1028
        },
        "outputId": "c29ee91d-db2f-409c-f1f2-87853ab05ade",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270331721,
          "user_tz": -60,
          "elapsed": 3964,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  10/1000: episode: 1, duration: 0.042s, episode steps: 10, steps per second: 236, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.990, 1.983], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  35/1000: episode: 2, duration: 0.016s, episode steps: 25, steps per second: 1550, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.280 [0.000, 1.000], mean observation: 0.024 [-2.133, 3.061], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  66/1000: episode: 3, duration: 0.019s, episode steps: 31, steps per second: 1641, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.015 [-2.130, 1.414], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  80/1000: episode: 4, duration: 0.009s, episode steps: 14, steps per second: 1573, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.100 [-2.025, 1.210], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  94/1000: episode: 5, duration: 0.009s, episode steps: 14, steps per second: 1584, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.076 [-2.570, 1.614], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 120/1000: episode: 6, duration: 0.304s, episode steps: 26, steps per second: 86, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.050 [-1.952, 3.082], loss: 0.642167, mean_absolute_error: 0.694914, mean_q: 0.360293\n",
            " 134/1000: episode: 7, duration: 0.046s, episode steps: 14, steps per second: 307, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.070 [-1.031, 1.621], loss: 0.540508, mean_absolute_error: 0.659335, mean_q: 0.478065\n",
            " 157/1000: episode: 8, duration: 0.080s, episode steps: 23, steps per second: 289, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.085 [-1.852, 0.962], loss: 0.516885, mean_absolute_error: 0.659902, mean_q: 0.569167\n",
            " 180/1000: episode: 9, duration: 0.079s, episode steps: 23, steps per second: 292, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.056 [-2.325, 1.405], loss: 0.429299, mean_absolute_error: 0.628117, mean_q: 0.677485\n",
            " 192/1000: episode: 10, duration: 0.044s, episode steps: 12, steps per second: 275, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.127 [-2.080, 1.188], loss: 0.413811, mean_absolute_error: 0.644944, mean_q: 0.780749\n",
            " 212/1000: episode: 11, duration: 0.067s, episode steps: 20, steps per second: 297, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.029 [-2.352, 3.322], loss: 0.407999, mean_absolute_error: 0.681430, mean_q: 0.820669\n",
            " 241/1000: episode: 12, duration: 0.097s, episode steps: 29, steps per second: 300, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.345 [0.000, 1.000], mean observation: -0.026 [-1.894, 2.459], loss: 0.392670, mean_absolute_error: 0.724528, mean_q: 0.959086\n",
            " 252/1000: episode: 13, duration: 0.035s, episode steps: 11, steps per second: 318, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.118 [-2.324, 1.422], loss: 0.439333, mean_absolute_error: 0.790947, mean_q: 1.018089\n",
            " 266/1000: episode: 14, duration: 0.052s, episode steps: 14, steps per second: 269, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.112 [-0.754, 1.448], loss: 0.408731, mean_absolute_error: 0.781260, mean_q: 1.077259\n",
            " 283/1000: episode: 15, duration: 0.056s, episode steps: 17, steps per second: 301, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.067 [-1.775, 2.784], loss: 0.403074, mean_absolute_error: 0.802900, mean_q: 1.145047\n",
            " 295/1000: episode: 16, duration: 0.041s, episode steps: 12, steps per second: 296, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-0.791, 1.566], loss: 0.415978, mean_absolute_error: 0.852549, mean_q: 1.185480\n",
            " 310/1000: episode: 17, duration: 0.055s, episode steps: 15, steps per second: 274, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.078 [-1.924, 1.171], loss: 0.387153, mean_absolute_error: 0.849717, mean_q: 1.260533\n",
            " 325/1000: episode: 18, duration: 0.050s, episode steps: 15, steps per second: 300, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.083 [-1.326, 0.817], loss: 0.403963, mean_absolute_error: 0.879646, mean_q: 1.294952\n",
            " 338/1000: episode: 19, duration: 0.043s, episode steps: 13, steps per second: 300, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.094 [-2.816, 1.776], loss: 0.399917, mean_absolute_error: 0.915888, mean_q: 1.354863\n",
            " 350/1000: episode: 20, duration: 0.048s, episode steps: 12, steps per second: 251, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.113 [-2.587, 1.610], loss: 0.402110, mean_absolute_error: 0.924888, mean_q: 1.418292\n",
            " 367/1000: episode: 21, duration: 0.057s, episode steps: 17, steps per second: 299, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.086 [-1.611, 0.833], loss: 0.399626, mean_absolute_error: 0.972336, mean_q: 1.467488\n",
            " 380/1000: episode: 22, duration: 0.047s, episode steps: 13, steps per second: 274, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.114 [-1.600, 0.969], loss: 0.405576, mean_absolute_error: 0.995437, mean_q: 1.515985\n",
            " 392/1000: episode: 23, duration: 0.041s, episode steps: 12, steps per second: 294, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.120 [-2.566, 1.520], loss: 0.400168, mean_absolute_error: 1.033636, mean_q: 1.564773\n",
            " 411/1000: episode: 24, duration: 0.061s, episode steps: 19, steps per second: 310, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.058 [-2.747, 1.778], loss: 0.407026, mean_absolute_error: 1.054153, mean_q: 1.657955\n",
            " 427/1000: episode: 25, duration: 0.063s, episode steps: 16, steps per second: 254, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.108 [-0.940, 1.831], loss: 0.383117, mean_absolute_error: 1.085275, mean_q: 1.701483\n",
            " 442/1000: episode: 26, duration: 0.049s, episode steps: 15, steps per second: 307, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.842, 1.712], loss: 0.378440, mean_absolute_error: 1.110717, mean_q: 1.749427\n",
            " 456/1000: episode: 27, duration: 0.047s, episode steps: 14, steps per second: 297, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.092 [-2.053, 1.174], loss: 0.380302, mean_absolute_error: 1.125398, mean_q: 1.812800\n",
            " 466/1000: episode: 28, duration: 0.034s, episode steps: 10, steps per second: 296, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.977, 3.037], loss: 0.400154, mean_absolute_error: 1.172904, mean_q: 1.849899\n",
            " 477/1000: episode: 29, duration: 0.038s, episode steps: 11, steps per second: 293, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.111 [-2.315, 1.386], loss: 0.374071, mean_absolute_error: 1.158547, mean_q: 1.884753\n",
            " 489/1000: episode: 30, duration: 0.047s, episode steps: 12, steps per second: 254, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-1.964, 1.156], loss: 0.364120, mean_absolute_error: 1.182284, mean_q: 1.942900\n",
            " 503/1000: episode: 31, duration: 0.055s, episode steps: 14, steps per second: 256, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.094 [-1.746, 2.693], loss: 0.394465, mean_absolute_error: 1.209872, mean_q: 1.992485\n",
            " 515/1000: episode: 32, duration: 0.042s, episode steps: 12, steps per second: 289, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.130 [-2.062, 1.180], loss: 0.414564, mean_absolute_error: 1.283200, mean_q: 2.048515\n",
            " 526/1000: episode: 33, duration: 0.037s, episode steps: 11, steps per second: 297, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.737, 1.785], loss: 0.407099, mean_absolute_error: 1.288013, mean_q: 2.069349\n",
            " 547/1000: episode: 34, duration: 0.067s, episode steps: 21, steps per second: 314, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.057 [-2.303, 1.414], loss: 0.399364, mean_absolute_error: 1.307103, mean_q: 2.158974\n",
            " 557/1000: episode: 35, duration: 0.036s, episode steps: 10, steps per second: 276, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.117 [-2.147, 1.327], loss: 0.438829, mean_absolute_error: 1.363763, mean_q: 2.251195\n",
            " 587/1000: episode: 36, duration: 0.108s, episode steps: 30, steps per second: 277, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.020 [-1.758, 1.157], loss: 0.446252, mean_absolute_error: 1.405563, mean_q: 2.303683\n",
            " 598/1000: episode: 37, duration: 0.044s, episode steps: 11, steps per second: 250, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.768, 2.771], loss: 0.436055, mean_absolute_error: 1.431764, mean_q: 2.352051\n",
            " 626/1000: episode: 38, duration: 0.093s, episode steps: 28, steps per second: 301, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.049 [-0.796, 1.678], loss: 0.420186, mean_absolute_error: 1.470809, mean_q: 2.458899\n",
            " 635/1000: episode: 39, duration: 0.032s, episode steps: 9, steps per second: 278, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.345, 1.411], loss: 0.448253, mean_absolute_error: 1.518444, mean_q: 2.517202\n",
            " 653/1000: episode: 40, duration: 0.059s, episode steps: 18, steps per second: 307, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.064 [-1.935, 1.314], loss: 0.450689, mean_absolute_error: 1.549541, mean_q: 2.571171\n",
            " 666/1000: episode: 41, duration: 0.044s, episode steps: 13, steps per second: 296, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.087 [-1.185, 1.828], loss: 0.468093, mean_absolute_error: 1.587471, mean_q: 2.610753\n",
            " 677/1000: episode: 42, duration: 0.045s, episode steps: 11, steps per second: 244, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.791, 0.987], loss: 0.465181, mean_absolute_error: 1.624210, mean_q: 2.664766\n",
            " 688/1000: episode: 43, duration: 0.048s, episode steps: 11, steps per second: 229, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.116 [-1.752, 0.998], loss: 0.437477, mean_absolute_error: 1.630151, mean_q: 2.742425\n",
            " 699/1000: episode: 44, duration: 0.037s, episode steps: 11, steps per second: 294, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.855, 1.776], loss: 0.473628, mean_absolute_error: 1.656866, mean_q: 2.786939\n",
            " 709/1000: episode: 45, duration: 0.036s, episode steps: 10, steps per second: 275, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.120 [-2.236, 1.395], loss: 0.570055, mean_absolute_error: 1.746310, mean_q: 2.868956\n",
            " 723/1000: episode: 46, duration: 0.046s, episode steps: 14, steps per second: 302, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.070 [-1.773, 2.659], loss: 0.449434, mean_absolute_error: 1.690574, mean_q: 2.834492\n",
            " 742/1000: episode: 47, duration: 0.059s, episode steps: 19, steps per second: 321, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.035 [-1.212, 1.695], loss: 0.541248, mean_absolute_error: 1.741215, mean_q: 2.894376\n",
            " 784/1000: episode: 48, duration: 0.142s, episode steps: 42, steps per second: 296, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.046 [-1.784, 2.276], loss: 0.471848, mean_absolute_error: 1.752955, mean_q: 2.921813\n",
            " 801/1000: episode: 49, duration: 0.056s, episode steps: 17, steps per second: 306, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.098 [-1.011, 1.913], loss: 0.477321, mean_absolute_error: 1.788993, mean_q: 2.982934\n",
            " 815/1000: episode: 50, duration: 0.044s, episode steps: 14, steps per second: 319, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.094 [-2.030, 1.185], loss: 0.493964, mean_absolute_error: 1.817128, mean_q: 3.058151\n",
            " 832/1000: episode: 51, duration: 0.056s, episode steps: 17, steps per second: 303, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.084 [-1.776, 1.019], loss: 0.465866, mean_absolute_error: 1.850431, mean_q: 3.107536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 883/1000: episode: 52, duration: 0.174s, episode steps: 51, steps per second: 293, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.569 [0.000, 1.000], mean observation: -0.039 [-2.349, 1.396], loss: 0.514085, mean_absolute_error: 1.942288, mean_q: 3.229285\n",
            " 904/1000: episode: 53, duration: 0.069s, episode steps: 21, steps per second: 304, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.092 [-1.017, 1.972], loss: 0.477412, mean_absolute_error: 1.983689, mean_q: 3.291059\n",
            " 917/1000: episode: 54, duration: 0.043s, episode steps: 13, steps per second: 301, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.803, 1.738], loss: 0.424549, mean_absolute_error: 2.006532, mean_q: 3.374639\n",
            " 932/1000: episode: 55, duration: 0.050s, episode steps: 15, steps per second: 301, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.105 [-0.967, 1.668], loss: 0.451293, mean_absolute_error: 2.018385, mean_q: 3.349550\n",
            " 946/1000: episode: 56, duration: 0.052s, episode steps: 14, steps per second: 268, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.082 [-1.997, 1.186], loss: 0.472696, mean_absolute_error: 2.063991, mean_q: 3.428491\n",
            " 965/1000: episode: 57, duration: 0.064s, episode steps: 19, steps per second: 295, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.076 [-1.026, 1.818], loss: 0.465786, mean_absolute_error: 2.075128, mean_q: 3.468150\n",
            "done, took 3.464 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5-xJqFVPf87m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "1c057472-1786-47e8-ffc3-3e0375430ae2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528270830441,
          "user_tz": -60,
          "elapsed": 841,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 10.000, steps: 10\n",
            "Episode 3: reward: 9.000, steps: 9\n",
            "Episode 4: reward: 10.000, steps: 10\n",
            "Episode 5: reward: 10.000, steps: 10\n",
            "Episode 6: reward: 11.000, steps: 11\n",
            "Episode 7: reward: 9.000, steps: 9\n",
            "Episode 8: reward: 10.000, steps: 10\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 9.000, steps: 9\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 9.000, steps: 9\n",
            "Episode 13: reward: 10.000, steps: 10\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 11.000, steps: 11\n",
            "Episode 17: reward: 9.000, steps: 9\n",
            "Episode 18: reward: 9.000, steps: 9\n",
            "Episode 19: reward: 11.000, steps: 11\n",
            "Episode 20: reward: 10.000, steps: 10\n",
            "Episode 21: reward: 10.000, steps: 10\n",
            "Episode 22: reward: 10.000, steps: 10\n",
            "Episode 23: reward: 9.000, steps: 9\n",
            "Episode 24: reward: 10.000, steps: 10\n",
            "Episode 25: reward: 8.000, steps: 8\n",
            "Episode 26: reward: 10.000, steps: 10\n",
            "Episode 27: reward: 8.000, steps: 8\n",
            "Episode 28: reward: 9.000, steps: 9\n",
            "Episode 29: reward: 10.000, steps: 10\n",
            "Episode 30: reward: 10.000, steps: 10\n",
            "Episode 31: reward: 9.000, steps: 9\n",
            "Episode 32: reward: 10.000, steps: 10\n",
            "Episode 33: reward: 9.000, steps: 9\n",
            "Episode 34: reward: 8.000, steps: 8\n",
            "Episode 35: reward: 10.000, steps: 10\n",
            "Episode 36: reward: 10.000, steps: 10\n",
            "Episode 37: reward: 9.000, steps: 9\n",
            "Episode 38: reward: 9.000, steps: 9\n",
            "Episode 39: reward: 9.000, steps: 9\n",
            "Episode 40: reward: 9.000, steps: 9\n",
            "Episode 41: reward: 9.000, steps: 9\n",
            "Episode 42: reward: 9.000, steps: 9\n",
            "Episode 43: reward: 10.000, steps: 10\n",
            "Episode 44: reward: 8.000, steps: 8\n",
            "Episode 45: reward: 9.000, steps: 9\n",
            "Episode 46: reward: 10.000, steps: 10\n",
            "Episode 47: reward: 10.000, steps: 10\n",
            "Episode 48: reward: 10.000, steps: 10\n",
            "Episode 49: reward: 9.000, steps: 9\n",
            "Episode 50: reward: 10.000, steps: 10\n",
            "mean: 9.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PoP-VNm4CjeC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}