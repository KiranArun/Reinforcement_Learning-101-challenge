{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_Notebook_no_display.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yKFL0v0acPCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "COPYRIGHT Â© 2018 Kiran Arun <kironni@gmail.com>"
      ]
    },
    {
      "metadata": {
        "id": "wApqhsthcQZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0f5qGxCG1L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "3ff1af64-5d0b-4696-fb44-6e4231ac827b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354874513,
          "user_tz": -60,
          "elapsed": 6247,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install gym\n",
        "!pip3 install keras-rl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\r\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.3)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Collecting keras-rl\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/a3/d39bd04750e9acf2205827e9331a5a01e45a618ad0fd00a0210d70b68025/keras-rl-0.4.0.tar.gz\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (0.19.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.12)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Running setup.py bdist_wheel for keras-rl ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/35/e9/0e/2eefa4b6383571cbeac2e615ff6d3cdffe32d0e4268e19d17d\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9R_EqIHcSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BRKs98b0CCYB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d44e6dea-0e37-46a0-d982-e3dfc77e2bd8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354878527,
          "user_tz": -60,
          "elapsed": 1601,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from rl import agents,memory,policy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2jyD4gwCCYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "40TkVe-RCCYJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "# number of steps to keep in experience buffer\n",
        "memory_limit = 400\n",
        "\n",
        "# discount value\n",
        "gamma = 0.99\n",
        "\n",
        "# how much to update target graph\n",
        "target_model_update = 1e-2\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# number of steps to sample from buffer to train on\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtaE4tirCCYM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50f94b36-bbac-48dc-c080-f2ed29822560",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354880163,
          "user_tz": -60,
          "elapsed": 495,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# setting environment\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NaQi_RlICCYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "tmRv8obfCCYQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e47eba15-8f25-43ea-8c65-8b27750a8dac",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354880726,
          "user_tz": -60,
          "elapsed": 512,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# create neural net\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# keep this as input layer\n",
        "model.add(layers.Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "# keep this as output layer\n",
        "model.add(layers.Dense(num_actions))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYXcjaq5CCYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "2fYkThdUCCYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2lX1KiyCCYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Memory"
      ]
    },
    {
      "metadata": {
        "id": "-jG_1P74CCYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# set experience buffer\n",
        "memory = memory.SequentialMemory(limit=memory_limit,\n",
        "                                    window_length=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld02_4hsCCYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "[info on choosing](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)"
      ]
    },
    {
      "metadata": {
        "id": "2fWIpcImCCYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set policy\n",
        "policy = policy.BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDuovy3BCCYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Agent\n",
        "\n",
        "choose from: https://github.com/keras-rl/keras-rl/tree/master/rl/agents\n",
        "\n",
        "and use: https://github.com/keras-rl/keras-rl/tree/master/examples"
      ]
    },
    {
      "metadata": {
        "id": "tiyRfNcaCCYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "model = agents.dqn.DQNAgent(model=model,\n",
        "                            nb_actions=num_actions,\n",
        "                            memory=memory,\n",
        "                            gamma=gamma,\n",
        "                            batch_size=batch_size,\n",
        "                            nb_steps_warmup=100,\n",
        "                            target_model_update=target_model_update,\n",
        "                            policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8bejhzpCCYl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer,\n",
        "            metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiLm78fBf7Gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "CAi8arNJCCYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1040
        },
        "outputId": "6235378e-b7b2-43fa-a041-e35fb394f838",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354907511,
          "user_tz": -60,
          "elapsed": 4310,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# train\n",
        "history = model.fit(env,\n",
        "                  nb_steps=1000,\n",
        "                  verbose=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 steps ...\n",
            "  12/1000: episode: 1, duration: 0.045s, episode steps: 12, steps per second: 269, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.124 [-0.766, 1.459], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  32/1000: episode: 2, duration: 0.013s, episode steps: 20, steps per second: 1498, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.784, 1.224], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  44/1000: episode: 3, duration: 0.008s, episode steps: 12, steps per second: 1430, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-0.955, 1.634], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  61/1000: episode: 4, duration: 0.013s, episode steps: 17, steps per second: 1274, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.081 [-0.821, 1.343], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  81/1000: episode: 5, duration: 0.013s, episode steps: 20, steps per second: 1493, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.344, 0.754], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  93/1000: episode: 6, duration: 0.008s, episode steps: 12, steps per second: 1443, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.041, 1.196], loss: --, mean_absolute_error: --, mean_q: --\n",
            " 107/1000: episode: 7, duration: 0.297s, episode steps: 14, steps per second: 47, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.112 [-2.304, 1.354], loss: 0.696453, mean_absolute_error: 0.859397, mean_q: 0.361634\n",
            " 117/1000: episode: 8, duration: 0.047s, episode steps: 10, steps per second: 213, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.150 [-1.364, 2.203], loss: 0.687876, mean_absolute_error: 0.877456, mean_q: 0.332698\n",
            " 132/1000: episode: 9, duration: 0.052s, episode steps: 15, steps per second: 291, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.083 [-1.543, 0.833], loss: 0.645906, mean_absolute_error: 0.871676, mean_q: 0.423400\n",
            " 148/1000: episode: 10, duration: 0.058s, episode steps: 16, steps per second: 278, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.092 [-2.011, 1.194], loss: 0.558205, mean_absolute_error: 0.863347, mean_q: 0.496075\n",
            " 158/1000: episode: 11, duration: 0.038s, episode steps: 10, steps per second: 264, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.155 [-2.074, 1.174], loss: 0.522616, mean_absolute_error: 0.822375, mean_q: 0.444248\n",
            " 178/1000: episode: 12, duration: 0.086s, episode steps: 20, steps per second: 233, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.039 [-1.482, 0.840], loss: 0.495930, mean_absolute_error: 0.833214, mean_q: 0.462802\n",
            " 207/1000: episode: 13, duration: 0.111s, episode steps: 29, steps per second: 260, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.061 [-0.959, 1.803], loss: 0.415086, mean_absolute_error: 0.768701, mean_q: 0.478203\n",
            " 229/1000: episode: 14, duration: 0.080s, episode steps: 22, steps per second: 277, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.091 [-1.694, 0.772], loss: 0.376536, mean_absolute_error: 0.767176, mean_q: 0.644888\n",
            " 240/1000: episode: 15, duration: 0.046s, episode steps: 11, steps per second: 240, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.347, 2.387], loss: 0.360485, mean_absolute_error: 0.769612, mean_q: 0.725051\n",
            " 253/1000: episode: 16, duration: 0.046s, episode steps: 13, steps per second: 285, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.117 [-1.032, 1.887], loss: 0.366555, mean_absolute_error: 0.813476, mean_q: 0.816757\n",
            " 261/1000: episode: 17, duration: 0.029s, episode steps: 8, steps per second: 277, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.155 [-1.389, 2.242], loss: 0.365633, mean_absolute_error: 0.831486, mean_q: 0.907548\n",
            " 271/1000: episode: 18, duration: 0.037s, episode steps: 10, steps per second: 273, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.114 [-1.209, 1.899], loss: 0.333041, mean_absolute_error: 0.829216, mean_q: 0.973786\n",
            " 325/1000: episode: 19, duration: 0.195s, episode steps: 54, steps per second: 277, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.183 [-1.079, 0.520], loss: 0.348820, mean_absolute_error: 0.876825, mean_q: 1.080310\n",
            " 348/1000: episode: 20, duration: 0.077s, episode steps: 23, steps per second: 298, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.065 [-2.220, 1.354], loss: 0.363138, mean_absolute_error: 0.963367, mean_q: 1.272992\n",
            " 365/1000: episode: 21, duration: 0.063s, episode steps: 17, steps per second: 271, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.081 [-1.508, 0.943], loss: 0.345647, mean_absolute_error: 0.989780, mean_q: 1.365498\n",
            " 385/1000: episode: 22, duration: 0.077s, episode steps: 20, steps per second: 261, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.073 [-0.744, 1.336], loss: 0.335827, mean_absolute_error: 0.985462, mean_q: 1.368613\n",
            " 396/1000: episode: 23, duration: 0.039s, episode steps: 11, steps per second: 282, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.167, 1.957], loss: 0.333776, mean_absolute_error: 1.016430, mean_q: 1.434304\n",
            " 407/1000: episode: 24, duration: 0.039s, episode steps: 11, steps per second: 279, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.366, 1.419], loss: 0.345222, mean_absolute_error: 1.030582, mean_q: 1.458150\n",
            " 427/1000: episode: 25, duration: 0.066s, episode steps: 20, steps per second: 302, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.074 [-0.971, 1.690], loss: 0.333472, mean_absolute_error: 1.043807, mean_q: 1.514579\n",
            " 445/1000: episode: 26, duration: 0.065s, episode steps: 18, steps per second: 277, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.039 [-1.842, 1.183], loss: 0.343560, mean_absolute_error: 1.106258, mean_q: 1.606414\n",
            " 459/1000: episode: 27, duration: 0.056s, episode steps: 14, steps per second: 248, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.109 [-1.151, 2.019], loss: 0.358501, mean_absolute_error: 1.119551, mean_q: 1.660622\n",
            " 481/1000: episode: 28, duration: 0.073s, episode steps: 22, steps per second: 302, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.019 [-1.607, 2.379], loss: 0.358528, mean_absolute_error: 1.167630, mean_q: 1.748104\n",
            " 501/1000: episode: 29, duration: 0.073s, episode steps: 20, steps per second: 273, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-0.768, 1.645], loss: 0.380723, mean_absolute_error: 1.220997, mean_q: 1.872583\n",
            " 511/1000: episode: 30, duration: 0.037s, episode steps: 10, steps per second: 270, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.107 [-1.611, 2.510], loss: 0.434916, mean_absolute_error: 1.292791, mean_q: 2.023376\n",
            " 533/1000: episode: 31, duration: 0.075s, episode steps: 22, steps per second: 292, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.069 [-1.522, 2.532], loss: 0.442691, mean_absolute_error: 1.381554, mean_q: 2.178695\n",
            " 566/1000: episode: 32, duration: 0.120s, episode steps: 33, steps per second: 274, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.034 [-1.157, 1.905], loss: 0.448402, mean_absolute_error: 1.449537, mean_q: 2.330260\n",
            " 578/1000: episode: 33, duration: 0.042s, episode steps: 12, steps per second: 286, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.522, 2.311], loss: 0.458831, mean_absolute_error: 1.517232, mean_q: 2.468201\n",
            " 587/1000: episode: 34, duration: 0.034s, episode steps: 9, steps per second: 268, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.765, 2.793], loss: 0.476412, mean_absolute_error: 1.589566, mean_q: 2.599688\n",
            " 617/1000: episode: 35, duration: 0.123s, episode steps: 30, steps per second: 244, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.367 [0.000, 1.000], mean observation: -0.010 [-1.603, 2.247], loss: 0.506464, mean_absolute_error: 1.606862, mean_q: 2.685307\n",
            " 631/1000: episode: 36, duration: 0.067s, episode steps: 14, steps per second: 210, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.276, 0.812], loss: 0.622652, mean_absolute_error: 1.683486, mean_q: 2.800715\n",
            " 643/1000: episode: 37, duration: 0.049s, episode steps: 12, steps per second: 245, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.133 [-1.728, 2.734], loss: 0.527456, mean_absolute_error: 1.664850, mean_q: 2.762875\n",
            " 665/1000: episode: 38, duration: 0.094s, episode steps: 22, steps per second: 233, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.102 [-0.820, 1.758], loss: 0.567892, mean_absolute_error: 1.709975, mean_q: 2.860705\n",
            " 678/1000: episode: 39, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.083 [-1.587, 2.425], loss: 0.537812, mean_absolute_error: 1.733239, mean_q: 2.951888\n",
            " 717/1000: episode: 40, duration: 0.158s, episode steps: 39, steps per second: 247, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.053 [-1.761, 2.117], loss: 0.690655, mean_absolute_error: 1.833922, mean_q: 3.076512\n",
            " 758/1000: episode: 41, duration: 0.164s, episode steps: 41, steps per second: 251, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.366 [0.000, 1.000], mean observation: -0.065 [-2.107, 2.724], loss: 0.674430, mean_absolute_error: 1.928473, mean_q: 3.304312\n",
            " 768/1000: episode: 42, duration: 0.048s, episode steps: 10, steps per second: 209, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.119 [-1.225, 1.971], loss: 0.578959, mean_absolute_error: 1.987045, mean_q: 3.510120\n",
            " 785/1000: episode: 43, duration: 0.066s, episode steps: 17, steps per second: 259, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.101 [-1.339, 2.365], loss: 0.555192, mean_absolute_error: 1.959550, mean_q: 3.451688\n",
            " 796/1000: episode: 44, duration: 0.051s, episode steps: 11, steps per second: 214, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.105 [-1.777, 2.763], loss: 0.995385, mean_absolute_error: 2.189673, mean_q: 3.774284\n",
            " 808/1000: episode: 45, duration: 0.049s, episode steps: 12, steps per second: 243, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.126 [-1.185, 2.067], loss: 0.668906, mean_absolute_error: 2.059248, mean_q: 3.656111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 821/1000: episode: 46, duration: 0.061s, episode steps: 13, steps per second: 214, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.096 [-1.768, 2.823], loss: 0.889339, mean_absolute_error: 2.162183, mean_q: 3.790550\n",
            " 832/1000: episode: 47, duration: 0.045s, episode steps: 11, steps per second: 242, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.133 [-1.143, 1.925], loss: 0.741677, mean_absolute_error: 2.164570, mean_q: 3.833999\n",
            " 855/1000: episode: 48, duration: 0.100s, episode steps: 23, steps per second: 229, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.039 [-1.916, 2.807], loss: 0.877889, mean_absolute_error: 2.238765, mean_q: 3.939010\n",
            " 871/1000: episode: 49, duration: 0.073s, episode steps: 16, steps per second: 220, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.054 [-1.597, 2.435], loss: 0.731504, mean_absolute_error: 2.260583, mean_q: 4.083200\n",
            " 886/1000: episode: 50, duration: 0.061s, episode steps: 15, steps per second: 245, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.100 [-1.789, 2.916], loss: 1.028940, mean_absolute_error: 2.369348, mean_q: 4.140732\n",
            " 905/1000: episode: 51, duration: 0.074s, episode steps: 19, steps per second: 256, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.075 [-1.345, 2.181], loss: 0.963343, mean_absolute_error: 2.367702, mean_q: 4.207563\n",
            " 914/1000: episode: 52, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.358, 2.331], loss: 1.229391, mean_absolute_error: 2.456281, mean_q: 4.306431\n",
            " 926/1000: episode: 53, duration: 0.055s, episode steps: 12, steps per second: 216, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.113 [-1.570, 2.562], loss: 0.921828, mean_absolute_error: 2.426485, mean_q: 4.368561\n",
            " 951/1000: episode: 54, duration: 0.099s, episode steps: 25, steps per second: 252, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-1.011, 1.952], loss: 0.908176, mean_absolute_error: 2.424440, mean_q: 4.379054\n",
            " 961/1000: episode: 55, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.111 [-1.195, 2.015], loss: 1.120478, mean_absolute_error: 2.498218, mean_q: 4.477388\n",
            " 989/1000: episode: 56, duration: 0.115s, episode steps: 28, steps per second: 243, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.018 [-1.725, 2.433], loss: 1.052439, mean_absolute_error: 2.498324, mean_q: 4.519949\n",
            " 1000/1000: episode: 57, duration: 0.045s, episode steps: 11, steps per second: 247, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.098 [-1.577, 2.385], loss: 0.970586, mean_absolute_error: 2.528943, mean_q: 4.585279\n",
            "done, took 3.963 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5-xJqFVPf87m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "6RsswqMpCCYq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "0c441316-b0f8-4dc0-c20d-3de8cdd6ca95",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527354911485,
          "user_tz": -60,
          "elapsed": 707,
          "user": {
            "displayName": "Kiran Arun",
            "photoUrl": "//lh4.googleusercontent.com/-8IeZZ41ybbo/AAAAAAAAAAI/AAAAAAAACkI/_LkIlaW8I6g/s50-c-k-no/photo.jpg",
            "userId": "105201401933439284277"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT EDIT\n",
        "\n",
        "# test\n",
        "hist = model.test(env, nb_episodes=50, visualize=False)\n",
        "print('mean:', np.mean(hist.history['episode_reward']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 8.000, steps: 8\n",
            "Episode 3: reward: 9.000, steps: 9\n",
            "Episode 4: reward: 8.000, steps: 8\n",
            "Episode 5: reward: 9.000, steps: 9\n",
            "Episode 6: reward: 10.000, steps: 10\n",
            "Episode 7: reward: 8.000, steps: 8\n",
            "Episode 8: reward: 10.000, steps: 10\n",
            "Episode 9: reward: 9.000, steps: 9\n",
            "Episode 10: reward: 8.000, steps: 8\n",
            "Episode 11: reward: 9.000, steps: 9\n",
            "Episode 12: reward: 8.000, steps: 8\n",
            "Episode 13: reward: 10.000, steps: 10\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 10.000, steps: 10\n",
            "Episode 17: reward: 10.000, steps: 10\n",
            "Episode 18: reward: 10.000, steps: 10\n",
            "Episode 19: reward: 8.000, steps: 8\n",
            "Episode 20: reward: 10.000, steps: 10\n",
            "Episode 21: reward: 9.000, steps: 9\n",
            "Episode 22: reward: 9.000, steps: 9\n",
            "Episode 23: reward: 9.000, steps: 9\n",
            "Episode 24: reward: 9.000, steps: 9\n",
            "Episode 25: reward: 9.000, steps: 9\n",
            "Episode 26: reward: 9.000, steps: 9\n",
            "Episode 27: reward: 10.000, steps: 10\n",
            "Episode 28: reward: 10.000, steps: 10\n",
            "Episode 29: reward: 10.000, steps: 10\n",
            "Episode 30: reward: 10.000, steps: 10\n",
            "Episode 31: reward: 8.000, steps: 8\n",
            "Episode 32: reward: 10.000, steps: 10\n",
            "Episode 33: reward: 9.000, steps: 9\n",
            "Episode 34: reward: 10.000, steps: 10\n",
            "Episode 35: reward: 10.000, steps: 10\n",
            "Episode 36: reward: 8.000, steps: 8\n",
            "Episode 37: reward: 9.000, steps: 9\n",
            "Episode 38: reward: 10.000, steps: 10\n",
            "Episode 39: reward: 9.000, steps: 9\n",
            "Episode 40: reward: 10.000, steps: 10\n",
            "Episode 41: reward: 9.000, steps: 9\n",
            "Episode 42: reward: 8.000, steps: 8\n",
            "Episode 43: reward: 9.000, steps: 9\n",
            "Episode 44: reward: 9.000, steps: 9\n",
            "Episode 45: reward: 9.000, steps: 9\n",
            "Episode 46: reward: 8.000, steps: 8\n",
            "Episode 47: reward: 10.000, steps: 10\n",
            "Episode 48: reward: 9.000, steps: 9\n",
            "Episode 49: reward: 8.000, steps: 8\n",
            "Episode 50: reward: 9.000, steps: 9\n",
            "mean: 9.16\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}